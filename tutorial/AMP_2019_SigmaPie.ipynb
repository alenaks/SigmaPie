{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: right\">\n",
    "    <i>\n",
    "        AMP 2019 (October 12) <br>\n",
    "        Alëna Aksënova\n",
    "    </i>\n",
    "</div>\n",
    "\n",
    "# _SigmaPie_ for subregular grammar induction\n",
    "\n",
    "## Subregular languages in phonology\n",
    "\n",
    "This toolkit is relevant for anyone who is working or going to work with subregualar grammars both from the perspectives of theoretical linguistics and formal language theory.\n",
    "\n",
    "**Why theoretical linguistics should be interested in formal language theory?** <br>\n",
    "_Formal language theory_ explains how potentially infinite stringsets, or _formal languages_,\n",
    "can be generalized to grammars encoding the desired patterns and what properties those\n",
    "grammars have. It also allows one to compare different grammars with respect to parameters such as expressivity.\n",
    "\n",
    "**Chomsky hierarchy** aligns main classes of formal languages with respect to their expressive power.\n",
    "  * **Regular** grammars are as powerful as finite state devices or regular expressions: they can \"count\" only until certain threshold (no $a^{n}b^{n}$ patterns);\n",
    "  * **Context-free** grammars have access to potentially infinite _stack_ that allows them to reproduce patterns that involve center embedding;\n",
    "  * **Mildly context-sensitive** grammars are powerful enough to handle some types of cross-serial dependencies such as copying;\n",
    "  * **Context sensitive** grammars are restricted to a finitely long [memory tape](https://en.wikipedia.org/wiki/Punched_tape) encoding the pattern;\n",
    "  * **Recursively enumerable** grammars are as powerful as any theoretically possible computer in this universe, they can use infinitely long memory tape.\n",
    "\n",
    "\n",
    "\n",
    "<img src=\"images/chomhier.png\" width=\"600\">\n",
    "\n",
    "\n",
    "Both phonology and morphology frequently display properties of regular languages.\n",
    "\n",
    "**Phonology** does not require the power of center-embedding. For example, consider a harmony where the first vowel agrees with the last vowel, second vowel agrees with the pre-last vowel, etc.\n",
    "    \n",
    "    GOOD: \"arugula\", \"tropicalization\", \"electrotelethermometer\", etc.\n",
    "    BAD:  any other word violating the rule.\n",
    "\n",
    "\n",
    "While it is a theoretically possible pattern, harmonies of that types are unattested in natural languages.\n",
    "\n",
    "**Morphology** avoids center-embedding as well. In [Aksënova et al. (2016)](https://www.aclweb.org/anthology/W16-2019) we show that it is possible to iterate prefixes with the meaning \"after\" in Russian. In Ilocano, where the same semantics is expressed via a circumfix, its iteration is prohibited.\n",
    "    \n",
    "    RUSSIAN: \"zavtra\" (tomorrow), \"posle-zavtra\" (the day after tomorrow), \n",
    "             \"posle-posle-zavtra\" (the day after the day after tomorrow), ...\n",
    "    ILOCANO: \"bigat\" (morning), \"ka-bigat-an\" (the next morning),\n",
    "             <*>\"ka-ka-bigat-an-an\" (the morning after the next one).\n",
    "\n",
    "\n",
    "Moreover, typological review of patterns shows that phonology and morphology do not require the full power of regular languages. As an example of an unattested pattern, [Heinz (2011)](http://jeffreyheinz.net/papers/Heinz-2011-CPF.pdf) provides a language where a word must have an even number of vowels to be well-formed.\n",
    "\n",
    "\n",
    "Regular languages can be sub-divided into another nested hierarchy of languages decreasing in their expressive power: **subregular hierarchy**.\n",
    "\n",
    "\n",
    "<img src=\"images/subreg.png\" width=\"250\">\n",
    "\n",
    "\n",
    "This tutorial and _SigmaPie_ toolkit currently contains functionality for the following classes:\n",
    "  * strictly piecewise (SP);\n",
    "  * strictly local (SL);\n",
    "  * tier-based strictly local (TSL);\n",
    "  * multiple tier-based strictly local (MTSL)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functionality of the toolkit\n",
    "\n",
    "  * **Learners** extract grammars from stringsets.\n",
    "  * **Scanners** evaluate strings with respect to a given grammar.\n",
    "  * **Sample generators** generate stringsets for a given grammar.\n",
    "  * **FSM constructors** translate subregular grammars to finite state machines.\n",
    "  * **Polarity converters** switch negative grammars to positive, and vice versa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd\n",
    "%cd Desktop/SigmaPie/code/\n",
    "\n",
    "from main import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Strictly piecewise languages\n",
    "\n",
    "**Negative strictly $k$-piecewise (SP)** grammars prohibit occurrence of sequences of $k$ symbols at an arbitrary distance from each other. The value of $k$ defines the size of the window of the grammar, or the length of the longest sequence that the grammar can prohibit. Alternatively, if the grammar is positive, it lists subsequences that are allowed in well-formed words of the language.\n",
    "\n",
    "    k = 2\n",
    "    POLARITY: negative\n",
    "    GRAMMAR:  ab, ba\n",
    "    LANGUAGE: accaacc, cbccc, cccacaaaa, ...\n",
    "              <*>accacba, <*>bcccacbb, <*>bccccccca, ...\n",
    "              \n",
    "              \n",
    "In phonology, an example of an SP pattren is _tone plateauing_ considered in [Jardine (2015,](https://adamjardine.net/files/jardinecomptone-short.pdf) [2016)](https://adamjardine.net/files/jardine2016dissertation.pdf).\n",
    "For example, in Luganda (Bantu) a low tone (L) cannot intervene in-between two high tones (H): L is changed to H in such configuration.\n",
    "The prosodic domain cannot have more than one stretch of H tones.\n",
    "\n",
    "**Luganda verb and noun combinations** (Hyman and Katamba (2010), cited by Jardine (2016))\n",
    "\n",
    "  * /tw-áa-mú-láb-a, walúsimbi/ $\\Rightarrow$ tw-áá-mu-lab-a, walúsimbi <br>\n",
    "    ‘we saw him, Walusimbi’ <br>\n",
    "    **HHLLL, LHLL**\n",
    "    \n",
    "  * /tw-áa-láb-w-a walúsimbi/ $\\Rightarrow$ tw-áá-láb-wá wálúsimbi <br>\n",
    "    ‘we were seen by Walusimbi’ <br>\n",
    "    **HHHHHHLL**\n",
    "    \n",
    "  * /tw-áa-láb-a byaa=walúsimbi/ $\\Rightarrow$ tw-áá-láb-á byáá-wálúsimbi <br>\n",
    "    ‘we saw those of Walusimbi’ <br>\n",
    "    **HHHHHHHHLL**\n",
    "    \n",
    "This pattern can be described using SP grammar $G_{SP_{neg}} = \\{HLH\\}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning tone plateauing pattern\n",
    "\n",
    "Let us say that `tone_plat` represents a \"toy\" example of tonal plateauing (TP) pattern."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "luganda = [\"LLLL\", \"HHLLL\", \"LHHHLL\", \"LLLLHHHH\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our goal will be to learn the generalization behind TP.\n",
    "\n",
    "Negative and positive SP grammars are implemented in the package in the `SP()` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tp_pattern = SP()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attributes of SP grammars\n",
    "  * `alphabet` (list) is the set of symbols that the grammar uses.\n",
    "  * `grammar` (list of tuples) is the list of allowed or prohibited substructures of the language;\n",
    "  * `k` (int) is the size of the locality window of the grammar, by default it is $2$;\n",
    "  * `data` (list of string) is the learning sample;\n",
    "  * `fsm` (FSM object) is the finite state device that corresponds to the grammar; in this case, the devide is FSM family constructed according to [Heinz&Rogers(2013)](https://www.aclweb.org/anthology/W13-3007).\n",
    "  \n",
    "The initial step is to define the training sample and the alphabet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tp_pattern.data = luganda\n",
    "tp_pattern.alphabet = [\"H\", \"L\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default, the locality window of the grammar is $2$ and the delimiters are \">\" and \"<\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Locality of the SP grammar:\", tp_pattern.k)\n",
    "print(\"Delimiters:\", tp_pattern.edges)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All these attributes can be directly accessed. For example, let us change the locality of the window from $2$ to $3$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tp_pattern.k = 3\n",
    "print(\"Locality of the SP grammar:\", tp_pattern.k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Methods for SP grammars\n",
    "  * `check_polarity()` and `switch_polarity()` display and changes the polarity of the grammar;\n",
    "  * `learn()` extracts prohibited or allowed subsequences from the training sample;\n",
    "  * `scan(string)` tells if a given string is well-formed with respect to a learned grammar;\n",
    "  * `extract_alphabet()` collects alphabet based on the provided data;\n",
    "  * `generate_sample(n, repeat)` generates $n$ strings based on the given grammar; by default, `repeat` is set to False, and repetitions of the generated strings are not allowed, but this parameter can be set to True;\n",
    "  * `fsmize()` creates the corresponding FSM family by following the steps outlined in [Heinz&Rogers(2013)](https://www.aclweb.org/anthology/W13-3007);\n",
    "  * `subsequences(string)` returns all $k$-piecewise subsequences of the given string;\n",
    "  * `generate_all_ngrams()` generates all possible strings of the length $k$ based on the provided alphabet.\n",
    "\n",
    "**Checking and changing polarity of the grammar**\n",
    "\n",
    "By default, the grammars are positive. The polarity can be checked by running the `check_polarity` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Polarity of the grammar:\", tp_pattern.check_polarity())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the polarity needs to be changed, this can be done using the `switch_polarity` method. It will automatically switch the grammar, if one is provided or already extracted, to the opposite one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tp_pattern.switch_polarity()\n",
    "print(\"Polarity of the grammar:\", tp_pattern.check_polarity())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Learning the SP grammar**\n",
    "\n",
    "Method `learn` extracts allowed or prohibited subsequences from the learning sample based on the polarity of the grammar and the locality window. Currently, $k=2$ and the grammar is negative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tp_pattern.learn()\n",
    "print(\"Extracted grammar:\", tp_pattern.grammar)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Indeed, it learned the TP pattern!\n",
    "\n",
    "$n$-grams are represented as tuples of strings, because in this case, elements of the alphabet are not restricted to characters, and it allows for other representations to be learned as well.\n",
    "\n",
    "**Scanning strings and telling if they are part of the language**\n",
    "\n",
    "Method `scan` takes  string as input and returns True or False depending on if the current string is contained in the language of the grammar:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tp = [\"HHHLLL\", \"L\", \"HHL\", \"LLHLLL\"]\n",
    "no_tp = [\"LLLLHLLLLH\", \"HLLLLLLH\", \"LLLHLLLHLLLHL\"]\n",
    "\n",
    "print(\"Tonal plateauing:\")\n",
    "for string in tp:\n",
    "    print(\"String\", string, \"is in L(G):\", tp_pattern.scan(string))\n",
    "    \n",
    "print(\"\\nNo tonal plateauing:\")\n",
    "for string in no_tp:\n",
    "    print(\"String\", string, \"is in L(G):\", tp_pattern.scan(string))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Generating a data sample**\n",
    "\n",
    "Based on the learned grammar, a data sample of the desired size can be generated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = tp_pattern.generate_sample(n = 10)\n",
    "print(\"Sample:\", sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Extracting subsequences**\n",
    "\n",
    "Finally, this toolkit can be used also in order to extract subsequences from the input word by feeding it to the `subsequences` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tp_pattern.k = 3\n",
    "print(\"k = 3:\", tp_pattern.subsequences(\"regular\"), \"\\n\")\n",
    "tp_pattern.k = 5\n",
    "print(\"k = 5:\", tp_pattern.subsequences(\"regular\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While SP languages capture multiple long-distance processes such as tone plateauings or some harmonies, they are unable to capture local processes, or blocking effect."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Strictly local languages\n",
    "\n",
    "**Negative strictly $k$-local (SL)** grammars prohibit occurrence of consecutive substrings consisting of up to $k$ symbols. The value of $k$ in this case, defines the longest substring that cannot be present in a well-formed string of a language. Positive SL grammars defines substrings that can be present in the language.\n",
    "\n",
    "Importantly, in order to define _first_ and _last_ elements, SL languages use delimiters (\">\" and \"<\") that indicate the beginning and the end of the string.\n",
    "\n",
    "    k = 2\n",
    "    POLARITY: positive\n",
    "    GRAMMAR:  >a, ab, ba, b<\n",
    "    LANGUAGE: ab, abab, abababab, ...\n",
    "              <*>babab, <*>abaab, <*>bababba, ...\n",
    "\n",
    "In phonology, very frequently changes involve adjacent segments, and the notion of locality is therefore extremely important. The discussion of local processes in phonology can be found in ([Chandlee 2014](http://dspace.udel.edu/bitstream/handle/19716/13374/2014_Chandlee_Jane_PhD.pdf)).\n",
    "\n",
    "\n",
    "**Russian word-final devoicing**\n",
    "\n",
    "In Russian, the final obstruent of a word cannot be voiced. <br>\n",
    "  * \"lug\" \\[luK\\] _meadow_ $\\Rightarrow$ \"lug-a\" \\[luGa\\] _of the meadow_\n",
    "  * \"luk\" \\[luK\\] _onion_ $\\Rightarrow$ \"luk-a\" \\[luKa\\] _of the onion_\n",
    "  * \"porog\" \\[paroK\\] _doorstep_ $\\Rightarrow$ \"porog-a\" \\[paroGa\\] _of the doorstep_\n",
    "  * \"porok\" \\[paroK\\] _vice_ $\\Rightarrow$ \"porok-a\" \\[paroKa\\] _of the vice_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning word-final devoicing\n",
    "\n",
    "Assume the following toy dataset where the following mapping is defined:\n",
    "  * \"a\" stands for a vowel;\n",
    "  * \"b\" stands for a voiced obstruent;\n",
    "  * \"p\" stands for any other consonant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "russian = [\"\", \"ababa\", \"babbap\", \"pappa\", \"pabpaapba\" \"aap\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this term, the Russian word-final devoicing generalization would be _\"do not have \"b\" at the end of the word\"_. However, in order to define \"beginning\" and \"end\", we need to use delimiters \">\" and \"<\".\n",
    "\n",
    "This pattern can then be described using SL grammar $G_{SL_{neg}} = \\{b<\\}$.\n",
    "\n",
    "Let us initialize a SL object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wf_devoicing = SL()\n",
    "wf_devoicing.data = russian"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attributes of SL grammars\n",
    "  * `alphabet` (list) is the set of symbols that the grammar uses.\n",
    "  * `grammar` (list of tuples) is the list of allowed or prohibited substructures of the language;\n",
    "  * `k` (int) is the size of the locality window of the grammar, by default it is $2$;\n",
    "  * `data` (list of string) is the learning sample;\n",
    "  * `edges` (list of two characters) are the delimiters used by the grammar, the default value is \">\" and \"<\";\n",
    "  * `fsm` (FSM object) is the finite state device that corresponds to the grammar.\n",
    "  \n",
    "### Methods defined for SL grammars\n",
    "  * `check_polarity()` and `switch_polarity()` display and changes the polarity of the grammar;\n",
    "  * `learn()` extracts prohibited or allowed subsequences from the training sample;\n",
    "  * `scan(string)` tells if a given string is well-formed with respect to a learned grammar;\n",
    "  * `extract_alphabet()` collects alphabet based on the provided data;\n",
    "  * `generate_sample(n, repeat)` generates $n$ strings based on the given grammar; by default, `repeat` is set to False, and repetitions of the generated strings are not allowed, but this parameter can be set to True;\n",
    "  * `fsmize()` creates the corresponding FSA;\n",
    "  * `clean_grammar()` removes useless $k$-grams from the grammar."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Extracting alphabet and learning SL grammar**\n",
    "\n",
    "As before, `learn()` method extracts dependencies from the data. It simply extracts $k$-grams of the indicated size from the data, and the default value of $k$ is $2$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wf_devoicing.learn()\n",
    "print(\"The grammar is\", wf_devoicing.grammar)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to automatically extract the alphabet from the data, it is possible to run `extract_alphabet()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Original value of the alphabet is\", wf_devoicing.alphabet)\n",
    "wf_devoicing.extract_alphabet()\n",
    "print(\"Modified value of the alphabet is\", wf_devoicing.alphabet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Changing polarity of the grammar**\n",
    "\n",
    "The grammar outputted above is positive. If we want to capture the pattern using restrictions rather then the allowed substrings, we can `switch_polarity()` of the grammar:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wf_devoicing.switch_polarity()\n",
    "print(\"The grammar is\", wf_devoicing.grammar)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Scanning strings**\n",
    "\n",
    "As before, `scan(string)` method returns True or False depending on the well-formedness of the given string with respect to the learned grammar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wfd = [\"apapap\", \"papa\", \"abba\"]\n",
    "no_wfd = [\"apab\", \"apapapb\"]\n",
    "\n",
    "print(\"Word-final devoicing:\")\n",
    "for string in wfd:\n",
    "    print(\"String\", string, \"is in L(G):\", wf_devoicing.scan(string))\n",
    "    \n",
    "print(\"\\nNo word-final devoicing:\")\n",
    "for string in no_wfd:\n",
    "    print(\"String\", string, \"is in L(G):\", wf_devoicing.scan(string))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Generating data samples**\n",
    "\n",
    "If the grammar is non-empty, the data sample can be generated in the same way as before for SP grammars: `generate_sample(n, repeat)`, where `n` is the number of examples that need to be generated, and `repeat` is a flag allowing or prohibiting repetitings of the same strings in the generated data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = wf_devoicing.generate_sample(5, repeat = False)\n",
    "print(sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Cleaning grammar**\n",
    "\n",
    "Potentially, a grammar that user provides can contain \"useless\" $k$-grams. For example, consider the following grammar:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sl = SL()\n",
    "sl.grammar = [(\">\", \"a\"), (\"b\", \"a\"), (\"a\", \"b\"), (\"b\", \"<\"),\n",
    "              (\">\", \"g\"), (\"f\", \"<\"), (\"t\", \"t\")]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This grammar contains $3$ useless bigrams:\n",
    "  \n",
    "  * `(\">\", \"g\")` can never be used because nothing can follow \"g\";\n",
    "  * `(\"f\", \"<\")` is useless because there is no way to start a string that would lead to \"f\";\n",
    "  * `(\"t\", \"t\")` has both problems listed above.\n",
    "  \n",
    "Method `clean_grammar()` detects and removes such $n$-grams by constructing a corresponding finite state machine, and trimming all inaccessible nodes of that FSM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Old grammar:\", sl.grammar)\n",
    "sl.clean_grammar()\n",
    "print(\"Clean grammar:\", sl.grammar)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even though SP and SL languages can capture a large portion of phonological well-formedness conditions, there are numerous examples of patterns that require increased complexity. For example, **harmony with a blocking effect** cannot be captured using SP grammars because they will \"miss\" a blocker, and cannot be encoded via SL grammars because they cannot be used for long-distance processes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tier-based strictly local languages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Acknowledgements** \n",
    "  * Thomas\n",
    "  * Jeff\n",
    "  * Ani"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Bibliography**\n",
    "\n",
    "  * Make a reference to Chomsky (?) for the Chomsky hierarchy\n",
    "  * Kaplan and Kay (1994)\n",
    "  * Karttunen et al. (1992)\n",
    "  * Shieber (1985)\n",
    "  * Heinz (2011)\n",
    "  * Aksenova et al (2016)\n",
    "  * Jardine 2015 and 2016\n",
    "  * Hyman and Katamba (2010)\n",
    "  * Jeffrey Heinz and James Rogers. 2013. Learning subregular classes of languages with factored deterministic automata. In Proceedings of the 13th Meeting on the Mathematics of Language (MoL 13), pages 64–71, Sofia, Bulgaria. Association for Computational Linguistics.\n",
    "  * Chandlee 2016"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
