{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: right\">\n",
    "    <i>\n",
    "        Alëna Aksënova <br>\n",
    "        Virtual NYI 2020 <br>\n",
    "        Topics in Compuational Linguistics\n",
    "    </i>\n",
    "</div>\n",
    " \n",
    "\n",
    "# Learning linguistic dependencies with [_SigmaPie_](https://pypi.org/project/SigmaPie/)\n",
    "\n",
    "\n",
    "Linguistic, and especially phonological and morphological generalizations can be roughly sub-divided to **well-formedness conditions**, and **transformational rules**.\n",
    "**(Formal) languages** are collections of strings, and **grammars** describe rules of those languages. Intuitively, it corresponds to well-formedness conditions.\n",
    "**Mappings** are collections of pairs that show strings \"before\" and \"after\" a transformation. Intuitively, they correspond to the transformations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To demonstrate automatic extraction of languages and mappings from data, I will be using my Python package _SigmaPie_. My dissertation [\"Tool-assisted induction of subregular languages and mappings\"](https://github.com/alenaks/Dissertation/blob/master/main.pdf) discusses the package, the underlying algorithms and experiments, and provides the overview of learning results based on artificial and natural language datasets. In this notebook, for the sake of simplicity, I will only use toy datasets.\n",
    "\n",
    "_SigmaPie_ implements several **subregular language classes** (strictly local, tier-based strictly local, multiple tier-based strictly local, and strictly piecewise). For every class, it provides a set of parameters that can be specified (locality window, alphabet, set of restrictions, polarity of the grammar, etc.) and functions that can be applied to members of those classes (evaluate a word, extract a grammar from data, construct the corresponding finite state automata, and others). These models can be employed to encode **well-formedness conditions**.\n",
    "\n",
    "Another type of finite models -- **transducers** -- are capable of capturing mappings, or **processes** rewriting the input sequence according to the encoded set of rules. Transduction learning algorithms extract the mapping from the observed input-output pairs. For example, OSTIA (Onward Subsequential Transducer Inference Algorithm) by [Oncina et. al (1993)](https://pdfs.semanticscholar.org/9058/01c8e75daacb27d70ccc3c0b587411b6d213.pdf) is a transduction learner that generalizes the training sample into a machine reading the input string symbol by symbol and outputting at every step the longest part of the output sequence known up to that moment.\n",
    "\n",
    "Finally, I demonstrate the functionality of _SigmaPie_ using the simplified datasets exhibiting harmonic and dissimilation patterns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accessing _SigmaPie_ toolkit\n",
    "\n",
    "The package can be installed via Python package manager `pip`, but if desired, the source code is available in [SigmaPie GitHub folder](https://github.com/alenaks/SigmaPie)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sigmapie\n",
      "Installing collected packages: sigmapie\n",
      "Successfully installed sigmapie-0.5\n"
     ]
    }
   ],
   "source": [
    "!pip3 install sigmapie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "You successfully loaded SigmaPie. \n",
      "\n",
      "Formal language classes and grammars available:\n",
      "\t* strictly piecewise: SP(alphabet, grammar, k, data, polar);\n",
      "\t* strictly local: SL(alphabet, grammar, k, data, edges, polar);\n",
      "\t* tier-based strictly local: TSL(alphabet, grammar, k, data, edges, polar, tier);\n",
      "\t* multiple tier-based strictly local: MTSL(alphabet, grammar, k, data, edges, polar).\n",
      "\n",
      "Alternatively, you can initialize a transducer: FST(states, sigma, gamma, initial, transitions, stout).\n",
      "Learning algorithm:\n",
      "\tOSTIA: ostia(sample, sigma, gamma).\n"
     ]
    }
   ],
   "source": [
    "from sigmapie import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell loads code that makes it easy to generate examples and datasets that I use further in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/alenaks/SigmaPie/tutorial/ex_generator\n"
     ]
    }
   ],
   "source": [
    "%cd ex_generator\n",
    "from ex_generator import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Languages, grammars and well-formedness conditions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well-formedness conditions restrict **shapes of strings** in some way.\n",
    "_Formal language theory_ explains how potentially infinite string sets, or _formal languages,_\n",
    "can be generalized to grammars encoding the desired patterns and what properties those\n",
    "grammars have.\n",
    "It also allows one to compare different grammars regarding parameters such as **expressivity**.\n",
    "\n",
    "\n",
    "**The Chomsky hierarchy** aligns the main classes of formal languages with respect to their expressive power [(Chomsky 1959)](http://www.cs.utexas.edu/~cannata/pl/Class%20Notes/Chomsky_1959%20On%20Certain%20Formal%20Properties%20of%20Grammars.pdf).\n",
    "\n",
    "  * **Regular** grammars are as powerful as finite-state devices or regular expressions, and they cannot produce patterns that require counting up to an arbitrary number (no $a^{n}b^{n}$ patterns);\n",
    "  * **Context-free** grammars have access to a potentially infinite _stack_ that allows them to produce patterns with nested dependencies, such as center embedding;\n",
    "  * **Mildly context-sensitive** grammars are powerful enough to handle cross-serial dependencies such as some types of copying;\n",
    "  * **Context-sensitive** grammars can handle unbounded copy languages and non-linear patterns such as $a^{2^{n}}$ for $n > 0$;\n",
    "  * **Recursively enumerable** grammars are as powerful as any theoretically possible computer and generate languages such as $a^n$, where $n \\in \\textrm{primes}$.\n",
    "\n",
    "\n",
    "\n",
    "<img src=\"images/chomhier.png\" width=\"600\">\n",
    "\n",
    "\n",
    "Both phonology and morphology frequently display properties of regular languages.\n",
    "\n",
    "**Phonology** does not require the power of center-embedding, which is a property of context-free languages. For example, consider a harmony where the first vowel agrees with the last vowel, the second vowel agrees with the pre-last one, etc. The following example shows this rule using English orthography.\n",
    "    \n",
    "    GOOD: \"arugula\", \"tropicalization\", \"electrotelethermometer\", etc.\n",
    "    BAD:  any other word violating the rule.\n",
    "\n",
    "\n",
    "While it is a theoretically possible pattern, harmonies of that type are unattested in natural languages.\n",
    "\n",
    "**Morphotactics** avoids center-embedding as well. In [Aksënova et al. (2016)](https://www.aclweb.org/anthology/W16-2019) we show that it is possible to iterate prefixes with the meaning \"after\" in Russian. In Ilocano, where the same semantics are expressed via a circumfix, its iteration is prohibited.\n",
    "    \n",
    "    RUSSIAN: \"zavtra\" (tomorrow), \"posle-zavtra\" (the day after tomorrow), \n",
    "             \"posle-posle-zavtra\" (the day after the day after tomorrow), ...\n",
    "    ILOCANO: \"bigat\" (morning), \"ka-bigat-an\" (the next morning),\n",
    "             <*>\"ka-ka-bigat-an-an\" (the morning after the next one)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subregular language classes\n",
    "\n",
    "\n",
    "Typological review of patterns shows that **phonology and morphology do not require the full power of regular languages**. As an example of an unattested pattern, [Heinz (2011)](http://jeffreyheinz.net/papers/Heinz-2011-CPF.pdf) provides a language where a word must have an even number of nasals to be well-formed.\n",
    "\n",
    "Regular languages can be sub-divided into another nested hierarchy of languages decreasing in their expressive power: **subregular hierarchy**.\n",
    "Among some of the most important characteristics of subregular languages is their learnability only from positive data: more powerful classes require negative input as well.\n",
    "\n",
    "\n",
    "<img src=\"images/subreg.png\" width=\"250\">\n",
    "\n",
    "\n",
    "The _SigmaPie_ toolkit currently contains functionality for the following subregular language and grammar classes:\n",
    "  * strictly piecewise (SP);\n",
    "  * strictly local (SL);\n",
    "  * tier-based strictly local (TSL);\n",
    "  * multiple tier-based strictly local (MTSL).\n",
    "  \n",
    "| Language | Dependencies it can handle                                    |\n",
    "|----------|---------------------------------------------------------------|\n",
    "| SL       | _only_ local dependencies                                     |\n",
    "| SP       | _only_ multiple long-distance dependencies _without_ blocking |\n",
    "| TSL      | long-distance dependencies _with_ blocking                    |\n",
    "| MTSL     | multiple long-distance dependencies _with_ blocking           |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "**Negative strictly piecewise (`SP`)** grammars prohibit the occurrence of sequences of symbols at an arbitrary distance from each other. Every SP grammar is associated with the value of $k$ that defines the size of the longest sequence that this grammar can prohibit. Alternatively, if the grammar is positive, it lists all subsequences that are allowed in well-formed words of the language. **SP grammars capture only long-distance dependencies that do not include blockers.**\n",
    "\n",
    "    k = 2\n",
    "    POLARITY: negative\n",
    "    GRAMMAR:  ab, ba\n",
    "    LANGUAGE: accaacc, cbccc, cccacaaaa, ...\n",
    "              <*>accacba, <*>bcccacbb, <*>bccccccca, ...\n",
    "\n",
    "**Negative strictly $k$-local (`SL`)** grammars prohibit the occurrence of consecutive substrings consisting of up to $k$ symbols. The value of $k$ defines the longest substring that cannot be present in a well-formed string of a language. Positive SL grammars define substrings that can be present in the language.\n",
    "To define _first_ and _last_ elements, SL languages use delimiters (\">\" and \"<\") that indicate the beginning and the end of the string. In phonology, changes involve adjacent segments very frequently, and the notion of locality is therefore extremely important. A discussion of local processes in phonology can be found in [(Chandlee 2014)](http://dspace.udel.edu/bitstream/handle/19716/13374/2014_Chandlee_Jane_PhD.pdf). **SL grammars only capture local dependencies.**\n",
    "\n",
    "    k = 2\n",
    "    POLARITY: positive\n",
    "    GRAMMAR:  >a, ab, ba, b<\n",
    "    LANGUAGE: ab, abab, abababab, ...\n",
    "              <*>babab, <*>abaab, <*>bababba, ...\n",
    "              \n",
    "**Negative tier-based strictly local (`TSL`)** grammars operate just like strictly local ones, but they have the power to _ignore_ a certain set of symbols completely. The set of symbols that are not ignored are called **tier** symbols, and the ones that do not matter for the well-formedness of strings are the **non-tier** ones [(Heinz et al. 2011)](https://pdfs.semanticscholar.org/b934/bfcc962f65e19ae139426668e8f8054e5616.pdf). The representation of a string with all non-tier symbols ignored is a _tier image_ of that string, and then the TSL grammars can be defined as _SL grammars that operate over a tier._ **TSL grammars capture a single long-distance dependency that can possibly include blockers.**\n",
    "\n",
    "    k = 2\n",
    "    POLARITY: negative\n",
    "    TIER:     b\n",
    "    GRAMMAR:  ><, bb\n",
    "    LANGUAGE: aaaabaaaa, b, aaaab, baaaa, aaabaaaa, ...\n",
    "              <*>aaaaa, <*>aaaabaabaa, <*>baaabaaa, ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Negative multiple tier-based strictly local (`MTSL`)** grammars are a conjunction of multiple TSL grammars: they consist of several tiers, and a set of restrictions is defined for every one of those tiers. In fact, there are numerous examples from the typological literature showing that there are phonological patterns of complexity which are beyond the power of TSL languages. One example could be any pattern where several long-distance dependencies affect different sets of elements, see [McMullin (2016)](https://www.dropbox.com/s/txmk4efif9f5bvb/McMullin_Dissertation_UBC.pdf?dl=0) and [Aksënova and Deshmukh (2018)](https://www.aclweb.org/anthology/W18-0307.pdf) for examples and discussions of those patterns. **MTSL grammars capture multiple long-distance dependencies that can possibly include blockers.**\n",
    "\n",
    "    k = 2\n",
    "    POLARITY:   negative\n",
    "    TIER_1:     a, o\n",
    "    GRAMMAR_1:  oa, ao\n",
    "    TIER_2:     p, b\n",
    "    GRAMMAR_2:  bp, pb\n",
    "    LANGUAGE:   obobo, apappp, opoppop, baaaaa, ...\n",
    "                <*>ppapboo, <*>oobbbab, <*>poobbap, ...\n",
    "\n",
    "The work here is based on **string representations**. The exemplified learning algorithms focus on **structural properties**, and are limited to non-probabilistic algorithms evaluating the well-formedness of input stings. This approach is currently extended to features ([Chandlee et al. 2019](https://www.aclweb.org/anthology/W19-5708/)) and autosegmental representations ([Chandlee and Jardine 2019](https://www.aclweb.org/anthology/Q19-1010/); [Rawski and Dolatian (to appear)](https://drive.google.com/file/d/19Ft6j7ta71uTTw3qkLRa6bqhR98caJGk/view)) in order to be more coherent with the representations used in linguistics. However, the statistical algorithms and the algorithms working with non-string-based representations are not implemented yet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vowel harmony and consonant harmony\n",
    "\n",
    "#### Pattern description\n",
    "\n",
    "In Bukusu, vowels agree in height, and a liquid \"l\" assimilates to \"r\" if followed by \"r\" somewhere further in the word [(Odden 1994)](https://www.jstor.org/stable/415830?seq=1#metadata_info_tab_contents).\n",
    "\n",
    "  * <b>r</b><i>ee</i>b-<i>e</i><b>r</b>- _ask-APPL_\n",
    "  * <b>l</b><i>i</i>m-<i>i</i><b>l</b>- _cultivate-APPL_\n",
    "  * <b>r</b><i>u</i>m-<i>i</i><b>r</b>- _send-APPL_\n",
    "  \n",
    "This pattern involves two long-distance assimilations: one of them affects vowels, and the other one is concerned with the consonants. To capture the big picture, we can simplify the dependency as follows: the two harmonic classes of vowels are mapped to \"a\" and \"o\", and the affected consonants are mapped to \"b\" and \"p\".\n",
    "\n",
    "    Good strings: aaabbabba, oppopooo, aapapapp, obooboboboobbb, ...\n",
    "    Bad strings:  <*>aabaoob, <*>paabab, <*>obabooo, ...\n",
    "    Generalization: if a string contains \"a\", it cannot contain \"o\", and vice versa;\n",
    "                    if a string contains \"p\", it cannot contain \"b\", and vice versa.\n",
    "                    \n",
    "<img src=\"images/mtsl.png\" width=\"530\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "harm_data = ['aabbaabb', 'abab', 'aabbab', 'abaabb', 'aabaab', 'abbabb', 'ooppoopp',\n",
    "             'opop', 'ooppop', 'opoopp', 'oopoop', 'oppopp', 'aappaapp', 'apap',\n",
    "             'aappap', 'apaapp', 'aapaap', 'appapp', 'oobboobb', 'obob', 'oobbob',\n",
    "             'oboobb', 'ooboob', 'obbobb', 'aabb', 'ab', 'aab', 'abb', 'oopp', 'op',\n",
    "             'oop', 'opp', 'oobb', 'ob', 'oob', 'obb', 'aapp', 'ap', 'aap', 'app',\n",
    "             'aaa', 'ooo', 'bbb', 'ppp', 'a', 'o', 'b', 'p', '']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initializing the SL, TSL, MTSL and SP classes\n",
    "\n",
    "This pattern in indeed an instance of an MTSL grammar, however, we can try to learn it using other grammars as well. `help` can be used to see what parameters and methods which subregular class implements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class MTSL in module sigmapie.mtsl_class:\n",
      "\n",
      "class MTSL(sigmapie.tsl_class.TSL)\n",
      " |  A class for tier-based strictly local grammars and languages.\n",
      " |  \n",
      " |  Attributes:\n",
      " |      alphabet (list): alphabet used in the language;\n",
      " |      grammar (list): the list of substructures;\n",
      " |      k (int): locality window;\n",
      " |      data (list): input data;\n",
      " |      edges (list): start- and end-symbols for the grammar;\n",
      " |      polar (\"p\" or \"n\"): polarity of the grammar;\n",
      " |      fsm (FSMFamily): a list of finite state machines that\n",
      " |          corresponds to the grammar;\n",
      " |      tier (list): list of tuples, where every tuple lists elements\n",
      " |          of some tier.\n",
      " |  Learning for k > 2 is not implemented: requires more theoretical work.\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      MTSL\n",
      " |      sigmapie.tsl_class.TSL\n",
      " |      sigmapie.sl_class.SL\n",
      " |      sigmapie.grammar.L\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, alphabet=None, grammar=None, k=2, data=None, edges=['>', '<'], polar='p')\n",
      " |      Initializes the TSL object.\n",
      " |  \n",
      " |  all_paths(self, dataset)\n",
      " |      Finds all paths that are present in a list of strings.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          dataset (list): a list of strings.\n",
      " |      Returns:\n",
      " |          list: a list of paths present in `dataset`.\n",
      " |  \n",
      " |  clean_grammar(self)\n",
      " |      Removes useless ngrams from the grammar.\n",
      " |      \n",
      " |      If negative, it just removes duplicates. If positive, it detects\n",
      " |      ngrams to which one cannot get     from the initial symbol and\n",
      " |      from which one cannot get     to the final symbol, and removes\n",
      " |      them.\n",
      " |  \n",
      " |  fsmize(self)\n",
      " |      Builds FSM family corresponding to the given grammar and saves in it\n",
      " |      the fsm attribute.\n",
      " |  \n",
      " |  gather_grammars(self, grammar)\n",
      " |      Gathers grammars with the same tier together.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          grammar (list): a representation of the learned grammar\n",
      " |              where there is a one-to-one mapping between tiers\n",
      " |              and bigrams.\n",
      " |      Returns:\n",
      " |          dict: a dictionary where keys are tiers and values are\n",
      " |              the restrictions imposed on those tiers.\n",
      " |  \n",
      " |  general_state_map(self, smaps)\n",
      " |      Generates a dictionary of transitions within all\n",
      " |      FSMs of the FSM family.\n",
      " |      Returns:\n",
      " |          dict: the dictionary of the form\n",
      " |              {\"keys\":[list of next symbols]}, where \n",
      " |              keys are (k-1)-long strings.\n",
      " |      Warning: the list of next symbols is tier-specific,\n",
      " |          so this estimates the rough options: refer to\n",
      " |          generate_item for the filtering of wrongly\n",
      " |          generated items.\n",
      " |  \n",
      " |  generate_item(self, tier_smap)\n",
      " |      Generates a well-formed string with respect to the given grammar.\n",
      " |      \n",
      " |      Returns:\n",
      " |          str: a well-formed string.\n",
      " |  \n",
      " |  generate_sample(self, n=10, repeat=True, safe=True)\n",
      " |      Generates a data sample of the required size, with or without\n",
      " |      repetitions depending on `repeat` value.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          n (int): the number of examples to be generated;\n",
      " |          repeat (bool): allows (rep=True) or prohibits (rep=False)\n",
      " |             repetitions within the list of generated items;\n",
      " |          safe (bool): automatically breaks out of infinite loops,\n",
      " |              for example, when the grammar cannot generate the\n",
      " |              required number of data items, and the repetitions\n",
      " |              are set to False.\n",
      " |      Returns:\n",
      " |          list: generated data sample.\n",
      " |  \n",
      " |  learn(self)\n",
      " |      Learns 2-local MTSL grammar for a given sample. The algorithm \n",
      " |      currently works only for k=2 and is based on MTSL2IA designed \n",
      " |      by McMullin, Aksenova and De Santo (2019). We are currently\n",
      " |      working on lifting the locality of the grammar to arbitrary k.\n",
      " |      Results:\n",
      " |          self.grammar is updated with a grammar of the following shape:\n",
      " |          {(tier_1):[bigrams_for_tier_1],\n",
      " |              ...\n",
      " |           (tier_n):[bigrams_for_tier_n]}\n",
      " |  \n",
      " |  map_restrictions_to_fsms(self)\n",
      " |      Maps restrictions to FSMs: based on the grammar, it creates a list\n",
      " |      of lists, where every sub-list has the following shape:\n",
      " |      \n",
      " |      [tier_n, restrictions_n, fsm_n]. Such sub-list is constructed\n",
      " |      for every single tier of the current MTSL grammar.\n",
      " |      Returns:\n",
      " |          [list, list, FSM]\n",
      " |              list: a list of current tier's symbols;\n",
      " |              list: a list of current tier's restrictions;\n",
      " |              FSM: a FSM corresponding to the current tier.\n",
      " |  \n",
      " |  opposite_polarity(self)\n",
      " |      Generates a grammar of the opposite polarity.\n",
      " |      \n",
      " |      Returns:\n",
      " |          dict: a dictionary containing the opposite ngram lists\n",
      " |              for every tier of the grammar.\n",
      " |  \n",
      " |  path(self, string)\n",
      " |      Collects a list of paths from a string.\n",
      " |      \n",
      " |      A path is a\n",
      " |      triplet <a, X, b>, where `a` is a symbol, `b` is a symbol\n",
      " |      that follows `a` in `string`, and `X` is a set of symbols\n",
      " |      in-between `a` and `b`.\n",
      " |      Arguments:\n",
      " |          string (str): a string paths of which need to be found.\n",
      " |      Returns:\n",
      " |          list: list of paths of `string`.\n",
      " |  \n",
      " |  scan(self, string)\n",
      " |      Scan string with respect to a given MTSL grammar.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          string (str): a string that needs to be scanned.\n",
      " |      Returns:\n",
      " |          bool: well-formedness of the string.\n",
      " |  \n",
      " |  switch_polarity(self)\n",
      " |      Changes polarity of the grammar, and rewrites grammar to the\n",
      " |      opposite one.\n",
      " |  \n",
      " |  tier_image(self, string)\n",
      " |      Creates tier images of a string with respect to the different\n",
      " |      tiers listed in the grammar.\n",
      " |      Returns:\n",
      " |          dict: a dictionary of the following shape:\n",
      " |              { (tier_1):\"string_image_given_tier_1\",\n",
      " |                  ...,\n",
      " |                (tier_n):\"string_image_given_tier_n\"\n",
      " |              }\n",
      " |  \n",
      " |  tier_state_maps(self)\n",
      " |      Generates a dictionary of transitions within the FSMs\n",
      " |      that correspond to the tier grammars.\n",
      " |      Returns:\n",
      " |          dict: the dictionary of the form\n",
      " |              {\n",
      " |               (tier_1):{\"keys\":[list of next symbols]},\n",
      " |               (tier_2):{\"keys\":[list of next symbols]},\n",
      " |                 ...\n",
      " |               (tier_n):{\"keys\":[list of next symbols]},\n",
      " |              }, where keys are (k-1)-long tier representations.\n",
      " |      Warning: the list of next symbols is tier-specific,\n",
      " |          so this estimates the rough options: refer to\n",
      " |          generate_item for the filtering of wrongly\n",
      " |          generated items.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sigmapie.tsl_class.TSL:\n",
      " |  \n",
      " |  learn_tier(self)\n",
      " |      This function determines which of the symbols used in the language\n",
      " |      are tier symbols, algorithm by Jardine & McMullin (2017).\n",
      " |      \n",
      " |      Updates tier attribute.\n",
      " |  \n",
      " |  state_map(self)\n",
      " |      Generates a dictionary of possible transitions in the FSM.\n",
      " |      Returns:\n",
      " |          dict: the dictionary of the form\n",
      " |              {\"keys\":[list of possible next symbols]}, where \n",
      " |              keys are (k-1)-long strings.\n",
      " |  \n",
      " |  test_insert(self, symbol, ngrams, ngrams_less)\n",
      " |      Tier presense test #1.\n",
      " |      \n",
      " |      For every (n-1)-gram ('x','y','z'),\n",
      " |      there must be n-grams of the type ('x','S','y','z') and\n",
      " |      ('x','y','S','z').\n",
      " |      Arguments:\n",
      " |          symbol (str): the symbol that is currently being tested;\n",
      " |          ngrams (list): the list of n-gramized input;\n",
      " |          ngrams_less (list): the list of (n-1)-gramized input.\n",
      " |      Returns:\n",
      " |          bool: True if a symbol passed the test, otherwise False.\n",
      " |  \n",
      " |  test_remove(self, symbol, ngrams, ngrams_more)\n",
      " |      Tier presense test #2.\n",
      " |      \n",
      " |      For every (n+1)-gram of the type\n",
      " |      ('x','S','y'), there must be an n-gram of the type ('x', 'y').\n",
      " |      Arguments:\n",
      " |          symbol (str): the symbol that is currently being tested;\n",
      " |          ngrams (list): the list of n-gramized input;\n",
      " |          ngrams_more (list): the list of (n+1)-gramized input.\n",
      " |      Returns:\n",
      " |          bool: True if a symbol passed the test, otherwise False.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sigmapie.sl_class.SL:\n",
      " |  \n",
      " |  annotate_string(self, string)\n",
      " |      Annotates the string with the start and end symbols.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          string (str): a string that needs to be annotated.\n",
      " |      Returns:\n",
      " |          str: annotated version of the string.\n",
      " |  \n",
      " |  ngramize_data(self)\n",
      " |      Creates set of n-grams based on the given data.\n",
      " |      \n",
      " |      Returns:\n",
      " |          list: collection of ngrams in the data.\n",
      " |  \n",
      " |  ngramize_item(self, item)\n",
      " |      This function n-gramizes a given string.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          item (str): a string that needs to be ngramized.\n",
      " |      Returns:\n",
      " |          list: list of ngrams from the item.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sigmapie.grammar.L:\n",
      " |  \n",
      " |  change_polarity(self, new_polarity=None)\n",
      " |      Changes the polarity of the grammar.\n",
      " |      \n",
      " |      Warning: it does not rewrite the grammar!\n",
      " |  \n",
      " |  check_polarity(self)\n",
      " |      Returns the polarity of the grammar (\"p\" or \"n\").\n",
      " |  \n",
      " |  extract_alphabet(self)\n",
      " |      Extracts alphabet from the given data or grammar and saves it into\n",
      " |      the 'alphabet' attribute.\n",
      " |      \n",
      " |      CAUTION: if not all symbols were used in the data or grammar,\n",
      " |              the result is not correct: update manually.\n",
      " |  \n",
      " |  generate_all_ngrams(self, symbols, k)\n",
      " |      Generates all possible ngrams of the length k based on the given\n",
      " |      alphabet.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          alphabet (list): alphabet;\n",
      " |          k (int): locality window (length of ngram).\n",
      " |      Returns:\n",
      " |          list: generated ngrams.\n",
      " |  \n",
      " |  well_formed_ngram(self, ngram)\n",
      " |      Tells if the given ngram is well-formed. An ngram is ill-formed if:\n",
      " |      \n",
      " |      * there is something in-between two start- or end-symbols\n",
      " |        ('>a>'), or\n",
      " |      * something is before start symbol or after the end symbol\n",
      " |        ('a>'), or\n",
      " |      * the ngram consists only of start- or end-symbols.\n",
      " |      Otherwise it is well-formed.\n",
      " |      Arguments:\n",
      " |          ngram (str): The ngram that needs to be evaluated.\n",
      " |      Returns:\n",
      " |          bool: well-formedness of the ngram.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from sigmapie.grammar.L:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(MTSL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step is to initialize $4$ subregular grammars for every one of the available classes and provide the training sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "sp_h = SP()\n",
    "sl_h = SL()\n",
    "tsl_h = TSL()\n",
    "mtsl_h = MTSL()\n",
    "\n",
    "sp_h.data = harm_data\n",
    "sl_h.data = harm_data\n",
    "tsl_h.data = harm_data\n",
    "mtsl_h.data = harm_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then simply fill the `L.alphabet` attributes by applying the method `L.extract_alphabet()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SP alphabet: ['a', 'b', 'o', 'p']\n",
      "SL alphabet: ['a', 'b', 'o', 'p']\n",
      "TSL alphabet: ['a', 'b', 'o', 'p']\n",
      "MTSL alphabet: ['a', 'b', 'o', 'p']\n"
     ]
    }
   ],
   "source": [
    "sp_h.extract_alphabet()\n",
    "sl_h.extract_alphabet()\n",
    "tsl_h.extract_alphabet()\n",
    "mtsl_h.extract_alphabet()\n",
    "\n",
    "print(\"SP alphabet:\", sp_h.alphabet)\n",
    "print(\"SL alphabet:\", sl_h.alphabet)\n",
    "print(\"TSL alphabet:\", tsl_h.alphabet)\n",
    "print(\"MTSL alphabet:\", mtsl_h.alphabet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Learning the grammars\n",
    "\n",
    "After the data and alphabet are established, we can extract the grammar with its corresponding complexity from every one of those classes using the method `L.learn()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "sp_h.learn()\n",
    "sl_h.learn()\n",
    "tsl_h.learn()\n",
    "mtsl_h.learn()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The learned **SP grammar** lists all possible $2$-long subsequences observed in the training sample. The sequences are represented as tuples instead of strings in order to avoid restricting the basic alphabet units to a single symbol."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive SP grammar: [('b', 'b'), ('a', 'b'), ('a', 'a'), ('b', 'a'), ('p', 'p'), ('o', 'p'), ('o', 'o'), ('p', 'o'), ('a', 'p'), ('p', 'a'), ('o', 'b'), ('b', 'o')]\n"
     ]
    }
   ],
   "source": [
    "print(\"Positive SP grammar:\", sp_h.grammar)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to express the same generalization using a negative grammar, we can `L.switch_polarity()` of the grammar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Polarity of the SP grammar: n\n",
      "SP grammar: [('a', 'o'), ('b', 'p'), ('o', 'a'), ('p', 'b')]\n"
     ]
    }
   ],
   "source": [
    "sp_h.switch_polarity()\n",
    "print(\"Polarity of the SP grammar:\", sp_h.check_polarity())\n",
    "print(\"SP grammar:\", sp_h.grammar)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The negative **SL grammar** contains the same set of restrictions as its SP counterpart. So, the same set of restrictions is detected, even though SP grammars express long-distance restrictions, whereas SL grammars only limit local relations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Polarity of the SL grammar: n\n",
      "SL grammar: [('a', 'o'), ('b', 'p'), ('o', 'a'), ('p', 'b')]\n"
     ]
    }
   ],
   "source": [
    "sl_h.switch_polarity()\n",
    "print(\"Polarity of the SL grammar:\", sl_h.check_polarity())\n",
    "print(\"SL grammar:\", sl_h.grammar)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TSL grammars** also express the same limitations over a tier as the previous two, and the tier includes all elements that participate in some sort of long-distance agreement. In this case, the tier includes all elements of the alphabet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TSL tier: ['a', 'b', 'o', 'p']\n",
      "Polarity of the TSL grammar: n\n",
      "TSL grammar: [('a', 'o'), ('b', 'p'), ('o', 'a'), ('p', 'b')]\n"
     ]
    }
   ],
   "source": [
    "print(\"TSL tier:\", tsl_h.tier)\n",
    "tsl_h.switch_polarity()\n",
    "print(\"Polarity of the TSL grammar:\", tsl_h.check_polarity())\n",
    "print(\"TSL grammar:\", tsl_h.grammar)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In **MTSL grammars**, the value of the attribute `L.grammar` is represented in the following way:\n",
    "\n",
    "    G = {\n",
    "            tier_1 (tuple): tier_1_restrictions (list),\n",
    "            tier_2 (tuple): tier_2_restrictions (list),\n",
    "                ...\n",
    "            tier_n (tuple): tier_n_restrictions (list)\n",
    "        }\n",
    "        \n",
    "The learned grammar detected two tiers: a tier of vowels `(\"a\", \"o\")` and a tier of consonants `(\"p\", \"b\")`. For every one of these tiers, it learned the corresponding set of restrictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MTSL tiers: [('a', 'o'), ('b', 'p')]\n",
      "Polarity of the MTSL grammar: n\n",
      "MTSL grammar: {('a', 'o'): [('a', 'o'), ('o', 'a')], ('b', 'p'): [('b', 'p'), ('p', 'b')]}\n"
     ]
    }
   ],
   "source": [
    "print(\"MTSL tiers:\", mtsl_h.tier)\n",
    "mtsl_h.switch_polarity()\n",
    "print(\"Polarity of the MTSL grammar:\", mtsl_h.check_polarity())\n",
    "print(\"MTSL grammar:\", mtsl_h.grammar)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generating samples\n",
    "\n",
    "Now, when all the grammars are learned, we can generate new data using the `L.generate_sample(n, repeat, safe)` method.\n",
    "\n",
    "The **SP-generated** data is consistent with the desired pattern: no \"a\" is followed by \"o\" in the generated strings of the language, and the consonantal agreement is preserved as well. SP grammar succeeded in capturing the pattern!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['', 'b', 'p', 'obboboooo', 'oop', 'ababbaabaa', 'babb', 'bbaaab', 'ppaaa', 'oo', 'a', 'o', 'ba', 'babba', 'oob', 'appppapaa', 'aba', 'apppa', 'aa', 'oopo', 'opo', 'pp', 'bob', 'opop', 'oppooopop']\n"
     ]
    }
   ],
   "source": [
    "print(sp_h.generate_sample(25, repeat=False, safe=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see below, **SL grammar** captured the local effect of the learned pattern (\"p\" is never adjacent to \"b\", \"o\" is never adjacent to \"a\", etc.), but it failed to generalize it to long-distance relations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['', 'b', 'p', 'poboo', 'oboobob', 'opappppa', 'appap', 'po', 'poo', 'oboppppabbooboboboboopp', 'aabab', 'poopoo', 'a', 'pa', 'o', 'oobb', 'ppppo', 'bo', 'obaab', 'abopo', 'opoboop', 'aa', 'apopo', 'ppabbopab', 'oba']\n"
     ]
    }
   ],
   "source": [
    "print(sl_h.generate_sample(25, repeat=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly to the errors made by the SL grammar, **TSL grammar** only captured a local dependency, and failed to generalize the pattern."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['', 'b', 'obo', 'p', 'bbbooopab', 'bop', 'aabobobb', 'a', 'pa', 'oppap', 'o', 'bappo', 'ba', 'oob', 'obaabapopapa', 'opoo', 'apab', 'aa', 'ooo', 'baapooo', 'oooppo', 'ob', 'apo', 'abapp', 'oppaaa']\n"
     ]
    }
   ],
   "source": [
    "print(tsl_h.generate_sample(25, repeat=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Increasing the locality window `L.k` did not help solve the issue: violations still occur, the only difference is that now they occur at a further distance away from each other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['', 'oopp', 'b', 'ababaaap', 'p', 'bbbab', 'oop', 'aaapaaababbooopoooob', 'aab', 'bb', 'oo', 'a', 'oppap', 'o', 'ppoobobbaaabb', 'oobbb', 'bbbbbobob', 'ppooopp', 'abbabab', 'aa', 'aaabbbaaapppaaabb', 'ap', 'pp', 'ab', 'apappaaaappaabbb']\n"
     ]
    }
   ],
   "source": [
    "tsl_h.k = 3\n",
    "tsl_h.learn()\n",
    "print(tsl_h.generate_sample(25, repeat=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And, as the next cell exemplifies, the **MTSL grammar** also successfully captured the double harmony pattern. Intuitively, MTSL grammar learned that on the tier of vowels, \"o\" and \"a\" should never be adjacent, and on the tier of consonants, restrictions \"pb\" and \"bp\" need to be imposed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['', 'p', 'opp', 'po', 'poo', 'bbabbb', 'poppoop', 'bbo', 'aapaapa', 'oo', 'pa', 'aap', 'o', 'baa', 'oobboo', 'bo', 'aapppa', 'abb', 'bbb', 'ap', 'ppappa', 'ob', 'ab', 'poopop', 'oopop']\n"
     ]
    }
   ],
   "source": [
    "print(mtsl_h.generate_sample(25, repeat=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overall, we can see that two grammars (SP and MTSL) successfully handled the double harmonic pattern.\n",
    "\n",
    "  * **SP solution:** within the same word, there cannot be two different vowels and two different consonants;\n",
    "  * **MTSL solution:** if we only look at vowels, all vowels that are adjacent to each other need to be the same, and similarly for consonants."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grammar learning exercise: liquid dissimilation\n",
    "\n",
    "In several languages (Latin, Georgian, a.o.), liquids tend to alternate. \n",
    "Consider the following Latin data: if the final liquid of the stem is \"l\", the adjectival affix is realized as \"aris\". And vice versa, if the final liquid is \"r\", the choice of the affix is \"alis\".\n",
    "\n",
    "  * mi<b>l</b>ita<b>r</b>is \\~ <*>mi<b>l</b>ita<b>l</b>is _\"military\"_\n",
    "  * f<b>l</b>o<b>r</b>a<b>l</b>is \\~ <*>f<b>l</b>o<b>r</b>a<b>r</b>is _\"floral\"_\n",
    "  * p<b>l</b>u<b>r</b>a<b>l</b>is \\~ <*>p<b>l</b>u<b>r</b>a<b>r</b>is _\"plural\"_\n",
    "  \n",
    "We can simplify this pattern by mapping the liquids to themselves, and any intervening vowel or consonant to `c`:\n",
    "\n",
    "    Good strings: clcccrccl, ccrccclcccrclc, lccccrlcccrcclccc, ...\n",
    "    Bad strings:  <*>ccrcccrccl, <*>clccrcr, <*>lccrccrc, ...\n",
    "    Generalization: the liquid following \"r\" needs to be \"l\", and vice versa;\n",
    "                    \"c\" is irrelevant for the ordering of the liquids. \n",
    "                    \n",
    "The following dataset exemplifies the pattern of liquid dissimilation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "liquid_data = [\"\", \"ccc\", \"lccrcccclcr\", \"lrl\", \"rcclc\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the cell below to explore which language type generalizes the liquid dissimilation pattern the best."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MEMO:\n",
    "# initialize the class and provide the data;\n",
    "# extract the alphabet from the data;\n",
    "# learn the grammar, possibly switch its polarity;\n",
    "# generate a sample and see if it complies with the generalization.\n",
    "\n",
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NB:** an improved version of the MTSL learning algorithm is coming soon!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mappings, transducers and processes\n",
    "\n",
    "Processes converting one representation to another, or, in other words, rewriting strings, can be encoded via another finite-state device -- **transducer**.\n",
    "For example, let us encode the following pattern from [de la Higuera (2014)](https://www.cambridge.org/core/books/grammatical-inference/CEEB229AC5A80DFC6436D860AC79434F): _the word-final \"a\" is rewritten as \"1\", other \"a\" are \"0\". All \"b\" are translated to \"1\"._\n",
    "See the following pairs, for example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "S = [(\"b\", \"1\"), (\"a\", \"1\"), (\"ab\", \"01\"), (\"abb\", \"011\"), (\"bb\", \"11\"), (\"aa\", \"01\"), \n",
    "     (\"aaa\", \"001\"), (\"aabaab\", \"001001\"), (\"aab\", \"001\"), (\"aaba\", \"0011\"), (\"aabaa\", \"00101\")]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This generalization can be captured using the following finite state transducer (FST).\n",
    "\n",
    "<img src=\"images/higtrans.png\" width=\"290\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two **states** of this FST: _\"e\"_ and \"a\", where _\"e\"_ is the unique initial state. The transition $q_e\\xrightarrow{\\text{b:1}}q_e$ reads \"b\" in the input, writes \"1\" to the output, and does not move the FST anywhere from the initial state. $q_e\\xrightarrow{\\text{a:e}}q_a$ reads an \"a\", moves the machine to another state, but does not write anything to the translation: it will depend on the next symbol.\n",
    "\n",
    "  * if the next symbol is \"a\", then it will write \"0\" to the translation: the previous \"a\" wasn't word-final;\n",
    "  * if the next symbol is \"b\", it writes \"01\", where \"0\" is obtained from the non-final \"a\", and \"1\" is the translation of \"b\";\n",
    "  * if there is no next symbol, the **state output function** defined by $e:e$ and $a:1$ writes \"1\" to the translation: in this case, \"a\" occurred in a word-final position.\n",
    "  \n",
    "The **input alphabet** of this machine is \"a\" and \"b\", and the **output alphabet** is \"0\" and \"1\".\n",
    "\n",
    "The FST module is only designed to work with transducers that read input strings symbol-by-symbol, or, in other words, **subsequential** transducers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoding an FST\n",
    "\n",
    "The class `FST` represents finite state transducers, and has the following functionality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class FST in module sigmapie.fst_object:\n",
      "\n",
      "class FST(builtins.object)\n",
      " |  A class representing finite state transducers.\n",
      " |  \n",
      " |  Attributes:\n",
      " |      Q (list): a list of states;\n",
      " |      Sigma (list): a list of symbols of the input alphabet;\n",
      " |      Gamma (list): a list of symbols of the output alphabet;\n",
      " |      qe (str): name of the unique initial state;\n",
      " |      E (list): a list of transitions;\n",
      " |      stout (dict): a collection of state outputs.\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, Sigma=None, Gamma=None)\n",
      " |      Initializes the FST object.\n",
      " |  \n",
      " |  copy_fst(self)\n",
      " |      Produces a deep copy of the current FST.\n",
      " |      \n",
      " |      Returns:\n",
      " |          T (FST): a copy of the current FST.\n",
      " |  \n",
      " |  rewrite(self, w)\n",
      " |      Rewrites the given string with respect to the rules represented in\n",
      " |      the current FST.\n",
      " |      \n",
      " |      Arguments:\n",
      " |          w (str): a string that needs to be rewritten.\n",
      " |      Outputs:\n",
      " |          str: the translation of the input string.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors defined here:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(FST)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then initialize the `FST` class as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "fst = FST()\n",
    "fst.Q = [\"\", \"a\"]\n",
    "fst.Sigma = [\"a\", \"b\"]\n",
    "fst.Gamma = [\"0\", \"1\"]\n",
    "fst.qe = \"\"\n",
    "fst.E = [['', 'b', '1', ''], ['', 'a', '', 'a'], ['a', 'b', '01', ''], ['a', 'a', '0', 'a']]\n",
    "fst.stout = {'': '', 'a': '1'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rewriting string using FSTs\n",
    "\n",
    "First, we can prepare several examples of strings that can be rewritten using the current FST."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated strings: ['aabbaab', 'aab', 'bab', 'a', 'abbb', 'bbb', 'bbba', 'aabababbaabba', 'bbbb', 'baabb']\n"
     ]
    }
   ],
   "source": [
    "inputs = SL(alphabet=[\"a\", \"b\"], grammar = [(\">\", \"<\")], polar=\"n\").generate_sample(10, repeat=False)\n",
    "print(\"Generated strings:\", inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, the method `rewrite` takes a string as input and returns its translation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aabbaab ---> 0011001\n",
      "aab ---> 001\n",
      "bab ---> 101\n",
      "a ---> 1\n",
      "abbb ---> 0111\n",
      "bbb ---> 111\n",
      "bbba ---> 1111\n",
      "aabababbaabba ---> 0010101100111\n",
      "bbbb ---> 1111\n",
      "baabb ---> 10011\n"
     ]
    }
   ],
   "source": [
    "for string in inputs:\n",
    "    print(string, \"--->\", fst.rewrite(string))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vowel harmony and consonant harmony, revisited\n",
    "\n",
    "Earlier in the notebook, we discussed a toy double harmonic pattern where a vowel within a word can be either \"o\" or \"a\", and the consonant can be either \"b\" or \"p\". \n",
    "The well-formed string were of the following type:\n",
    "    \n",
    "    [\"baabb\", \"bbboo\", \"ppaaa\", \"ppppp\"]\n",
    "    \n",
    "Now, let us capture the feature changing process instead of defining the well-formedness conditions. Assuming that the spreading moves left to right, we can then mask all non-initial mentions of vowels and consonants inthe words.\n",
    "    \n",
    "    [(\"baABB\", \"baabb\"), (\"bBBoA\", \"bbboo\"), (\"pBaAA\", \"ppaaa\"), (\"pBBBB\", \"ppppp\")]\n",
    "    \n",
    "First of all, let us start by defining toy harmonic classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "specifications = {(\"a\", \"o\"):\"A\", (\"b\", \"p\"):\"B\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let us generate the training sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 first pairs of S:\n",
      " [('aAAbB', 'aaabb'), ('opABB', 'opopp'), ('bBBoA', 'bbboo'), ('oAApB', 'ooopp'), ('baBAA', 'babaa')]\n"
     ]
    }
   ],
   "source": [
    "num_examples = 10\n",
    "len_examples = 5\n",
    "S = generate_pairs(num_examples, len_examples, specifications)\n",
    "\n",
    "show = 5\n",
    "print(show, \"first pairs of S:\\n\", S[:show])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning with OSTIA\n",
    "\n",
    "Transducers can be extracted from the input and output pairs automatically.\n",
    "There are several algorithms that rely on different properties and assumptions.\n",
    "Among the most widely used and discussed learners, there are:\n",
    "\n",
    "  * **OSTIA** (Onward Subsequential Transducer Inference Algorithm) by [Oncina, Garcia and Vidal (1993)](https://pdfs.semanticscholar.org/9058/01c8e75daacb27d70ccc3c0b587411b6d213.pdf) and [de la Higuera (2014)](https://www.cambridge.org/core/books/grammatical-inference/CEEB229AC5A80DFC6436D860AC79434F);\n",
    "  * **ISLFLA** (Input Strictly Local Function Learning Algorithm) by [Chandlee, Eyraud and Heinz (2014)](https://hal.archives-ouvertes.fr/hal-01193047/document);\n",
    "  * **OSLFIA** (Output Strictly Local Function Inference Algorithm) by [Chandlee, Eyraud and Heinz (2015)](https://www.aclweb.org/anthology/W15-2310.pdf), and others.\n",
    "  \n",
    "#### Setting up the learner\n",
    "  \n",
    "The following section discusses OSTIA learning algorithm, while some of the other ones will be added to _SigmaPie_ codebase soon."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function ostia in module sigmapie.ostia:\n",
      "\n",
      "ostia(S, Sigma, Gamma)\n",
      "    This function implements OSTIA (Onward Subsequential Transduction\n",
      "    Inference Algorithm).\n",
      "    \n",
      "    Arguments:\n",
      "        S (list): a list of pairs (o, t), where `o` is the original\n",
      "            string, and `t` is its translation;\n",
      "        Sigma (list): the input alphabet;\n",
      "        Gamma (list): the output alphabet.\n",
      "    Returns:\n",
      "        FST: a transducer defining the mapping.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(ostia)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let us provide the necessary input to OSTIA and save the resulting machine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "S = generate_pairs(1500, 5, specifications)\n",
    "Sigma = [\"a\", \"o\", \"A\", \"b\", \"p\", \"B\"]\n",
    "Gamma = [\"a\", \"o\", \"b\", \"p\"]\n",
    "T = ostia(S, Sigma, Gamma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a step-by-step implementation of OSTIA, click [here](https://github.com/alenaks/OSTIA/blob/master/ostia.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluating the performance\n",
    "\n",
    "In order to evaluate the performance of the obtained automaton, we can generate more input forms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pBBBB ---> ppppp\n",
      "paBBB ---> pappp\n",
      "pBBBB ---> ppppp\n",
      "oAAAb ---> oooob\n",
      "oAAAp ---> oooop\n",
      "pBBBo ---> ppppo\n",
      "pBBBB ---> ppppp\n",
      "paAAA ---> paaaa\n",
      "paABA ---> paapa\n",
      "abBBA ---> abbba\n",
      "aAApA ---> aaapa\n",
      "oApAB ---> oopop\n",
      "opBAB ---> oppop\n",
      "paBBB ---> pappp\n",
      "boABA ---> boobo\n"
     ]
    }
   ],
   "source": [
    "test = mask_words(generate_words(15, 5, specifications), specifications)\n",
    "for w in test:\n",
    "    print(w, \"--->\", T.rewrite(w))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As one can see, the performance of OSTIA largely depends on the size of the training sample and the length of the words. \n",
    "For example, if the length of words is set to $5$, we need to observe at least $200$ examples in order to see the stably correct outputs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpretability\n",
    "\n",
    "After the transducer was extracted, it is possible to explore its structure by simply viweing the list of transitions, states, and state outputs of that machine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "States: ['', 'o', 'p', 'po']\n",
      "State outputs: {'': '', 'o': '', 'p': '', 'po': ''}\n",
      "\n",
      "Transitions: [('', 'b', 'b', ''), ('', 'a', 'a', ''), ('', 'o', 'o', 'o'), ('o', 'b', 'b', 'o'), ('', 'p', 'p', 'p'), ('p', 'a', 'a', 'p'), ('p', 'o', 'o', 'po'), ('po', 'B', 'p', 'po'), ('o', 'A', 'o', 'o'), ('p', 'B', 'p', 'p'), ('o', 'p', 'p', 'po'), ('po', 'A', 'o', 'po'), ('o', 'B', 'b', 'o'), ('', 'B', 'b', ''), ('', 'A', 'a', ''), ('p', 'A', 'a', 'p')]\n"
     ]
    }
   ],
   "source": [
    "print(\"States:\", T.Q)\n",
    "print(\"State outputs:\", T.stout)\n",
    "print(\"\\nTransitions:\", T.E)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, one of the transducers that was build based on the harmonic pairs had the following list of states and transitions. (State outputs are not important in this case: all of them output $\\epsilon$.)\n",
    "\n",
    "    T.Q = ['', 'o', 'b', 'ob']\n",
    "\n",
    "    T.E = [['', 'p', 'p', ''], ['', 'a', 'a', ''], ['', 'o', 'o', 'o'],\n",
    "           ['o', 'p', 'p', 'o'], ['', 'b', 'b', 'b'], ['b', 'a', 'a', 'b'],\n",
    "           ['o', 'A', 'o', 'o'], ['o', 'b', 'b', 'ob'], ['ob', 'A', 'o', 'ob'],\n",
    "           ['b', 'B', 'b', 'b'], ['b', 'o', 'o', 'ob'], ['ob', 'B', 'b', 'ob'],\n",
    "           ['o', 'B', 'p', 'o'], ['', 'B', 'p', ''], ['', 'A', 'a', ''],\n",
    "           ['b', 'A', 'a', 'b']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then visualize this FST, showing that the results of transduction learning are interpretable.\n",
    "\n",
    "<img src=\"images/double_harm_improved.png\" width=\"450\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "_SigmaPie_ is a package for subregular and subsequential grammar induction.\n",
    "**Subregular languages and grammar** capture the well-formedness conditions, and **subsequential transducers** are capable of encoding processes, or mappings.\n",
    "\n",
    "I exemplified the performance of the implemented learning algorithms based on the toy assimilation and dissimilation patterns.\n",
    "After the FST corresponding to a training sample is built, it is possible to look at the shape of the learned machine, therefore making this approach to grammar construction **fully interpretable**. For further details, please refer to my [dissertation](https://github.com/alenaks/Dissertation/blob/master/main.pdf)! :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Acknowledgments** \n",
    "\n",
    "I am very grateful to [_Thomas Graf_](https://thomasgraf.net/), [_Jeffrey Heinz_](http://jeffreyheinz.net/), [_Aniello De Santo_](https://aniellodesanto.github.io/about/) and _Ayla Karakaş_ whose input on different parts of this project was extremely helpful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
