{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: right\">\n",
    "    <i>\n",
    "        Alëna Aksënova\n",
    "    </i>\n",
    "</div>\n",
    " \n",
    "\n",
    "# [_SigmaPie_](https://github.com/alenaks/SigmaPie) for subregular grammar induction\n",
    "\n",
    "This toolkit is relevant for anyone who is working or going to work with subregular grammars both from the perspectives of theoretical linguistics and formal language theory.\n",
    "\n",
    "## Importance of formal languages\n",
    "\n",
    "**Why theoretical linguists might be interested in formal language theory?** <br>\n",
    "_Formal language theory_ explains how potentially infinite string sets, or _formal languages_,\n",
    "can be generalized to grammars encoding the desired patterns and what properties those\n",
    "grammars have. It also allows one to compare different grammars regarding parameters such as **expressivity**.\n",
    "\n",
    "\n",
    "**The Chomsky hierarchy** aligns the main classes of formal languages with respect to their expressive power [(Chomsky 1959)](http://www.cs.utexas.edu/~cannata/pl/Class%20Notes/Chomsky_1959%20On%20Certain%20Formal%20Properties%20of%20Grammars.pdf).\n",
    "\n",
    "  * **Regular** grammars are as powerful as finite-state devices or regular expressions, and they cannot produce patterns that require counting up to an arbitrary number (no $a^{n}b^{n}$ patterns);\n",
    "  * **Context-free** grammars have access to a potentially infinite _stack_ that allows them to reproduce patterns that involve center embedding;\n",
    "  * **Mildly context-sensitive** grammars are powerful enough to handle cross-serial dependencies such as some types of copying;\n",
    "  * **Context-sensitive** grammars can handle non-linear patterns such as $a^{2^{n}}$ for $n > 0$;\n",
    "  * **Recursively enumerable** grammars are as powerful as any theoretically possible computer and generate languages such as $a^n$, where $n \\in \\textrm{primes}$.\n",
    "\n",
    "\n",
    "\n",
    "<img src=\"images/chomhier.png\" width=\"600\">\n",
    "\n",
    "\n",
    "Both phonology and morphology frequently display properties of regular languages.\n",
    "\n",
    "**Phonology** does not require the power of center-embedding, which is a property of context-free languages. For example, consider a harmony where the first vowel agrees with the last vowel, the second vowel agrees with the pre-last one, etc. The following example shows this rule using English orthography.\n",
    "    \n",
    "    GOOD: \"arugula\", \"tropicalization\", \"electrotelethermometer\", etc.\n",
    "    BAD:  any other word violating the rule.\n",
    "\n",
    "\n",
    "While it is a theoretically possible pattern, harmonies of that type are unattested in natural languages.\n",
    "\n",
    "**Morphology** avoids center-embedding as well. In [Aksënova et al. (2016)](https://www.aclweb.org/anthology/W16-2019) we show that it is possible to iterate prefixes with the meaning \"after\" in Russian. In Ilocano, where the same semantics are expressed via a circumfix, its iteration is prohibited.\n",
    "    \n",
    "    RUSSIAN: \"zavtra\" (tomorrow), \"posle-zavtra\" (the day after tomorrow), \n",
    "             \"posle-posle-zavtra\" (the day after the day after tomorrow), ...\n",
    "    ILOCANO: \"bigat\" (morning), \"ka-bigat-an\" (the next morning),\n",
    "             <*>\"ka-ka-bigat-an-an\" (the morning after the next one)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Subregular language classes\n",
    "\n",
    "\n",
    "Typological review of patterns shows that **phonology and morphology do not require the full power of regular languages**. As an example of an unattested pattern, [Heinz (2011)](http://jeffreyheinz.net/papers/Heinz-2011-CPF.pdf) provides a language where a word must have an even number of nasals to be well-formed. Regular languages can be sub-divided into another nested hierarchy of languages decreasing in their expressive power: **subregular hierarchy**.\n",
    "Among some of the most important characteristics of subregular languages is their learnability only from positive data: more powerful classes require negative input as well.\n",
    "\n",
    "\n",
    "<img src=\"images/subreg.png\" width=\"250\">\n",
    "\n",
    "\n",
    "The _SigmaPie_ toolkit currently contains functionality for the following subregular language and grammar classes:\n",
    "  * strictly piecewise (SP);\n",
    "  * strictly local (SL);\n",
    "  * tier-based strictly local (TSL);\n",
    "  * multiple tier-based strictly local (MTSL).\n",
    "  \n",
    "| Language | Dependencies it can handle                                    |\n",
    "|----------|---------------------------------------------------------------|\n",
    "| SL       | _only_ local dependencies                                     |\n",
    "| SP       | _only_ multiple long-distance dependencies _without_ blocking |\n",
    "| TSL      | long-distance dependencies _with_ blocking                    |\n",
    "| MTSL     | multiple long-distance dependencies _with_ blocking           |\n",
    "\n",
    "\n",
    "The work here is based on **string representations**. The exemplified learning algorithms focus on **structural properties**, and are limited to non-probabilistic algorithms evaluating the well-formedness of input stings. This approach is currently extended to features ([Chandlee et al. 2019](https://www.aclweb.org/anthology/W19-5708/)) and autosegmental representations ([Chandlee and Jardine 2019](https://www.aclweb.org/anthology/Q19-1010/); [Rawski and Dolatian (to appear)](https://drive.google.com/file/d/19Ft6j7ta71uTTw3qkLRa6bqhR98caJGk/view)) in order to be more coherent with the representations used in linguistics. However, the statistical algorithms and the algorithms working with non-string-based representations are not implemented yet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linguistically-inspired examples\n",
    "\n",
    "In this section, I present $2$ examples that I will later use to exemplify the functionality of the package:\n",
    "\n",
    "  * toy double harmony example;\n",
    "  * toy tone plateauing example.\n",
    "  \n",
    "### Vowel harmony and consonant harmony\n",
    "\n",
    "In Bukusu, vowels agree in height, and a liquid \"l\" assimilates to \"r\" if followed by \"r\" somewhere further in the word [(Odden 1994)](https://www.jstor.org/stable/415830?seq=1#metadata_info_tab_contents).\n",
    "\n",
    "  * <b>r</b><i>ee</i>b-<i>e</i><b>r</b>- _ask-APPL_\n",
    "  * <b>l</b><i>i</i>m-<i>i</i><b>l</b>- _cultivate-APPL_\n",
    "  * <b>r</b><i>u</i>m-<i>i</i><b>r</b>- _send-APPL_\n",
    "  \n",
    "This pattern involves two long-distance assimilations: one of them affects vowels, and the other one is concerned with the consonants. To capture the big picture, we can simplify the dependency as follows: the two harmonic classes of vowels are mapped to \"a\" and \"o\", and the affected consonants are mapped to \"b\" and \"p\".\n",
    "\n",
    "    Good strings: aaabbabba, oppopooo, aapapapp, obooboboboobbb, ...\n",
    "    Bad strings:  <*>aabaoob, <*>paabab, <*>obabooo, ...\n",
    "    Generalization: if a string contains \"a\", it cannot contain \"o\", and vice versa;\n",
    "                    if a string contains \"p\", it cannot contain \"b\", and vice versa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "harm_data = ['aabbaabb', 'abab', 'aabbab', 'abaabb', 'aabaab', 'abbabb', 'ooppoopp',\n",
    "             'opop', 'ooppop', 'opoopp', 'oopoop', 'oppopp', 'aappaapp', 'apap',\n",
    "             'aappap', 'apaapp', 'aapaap', 'appapp', 'oobboobb', 'obob', 'oobbob',\n",
    "             'oboobb', 'ooboob', 'obbobb', 'aabb', 'ab', 'aab', 'abb', 'oopp', 'op',\n",
    "             'oop', 'opp', 'oobb', 'ob', 'oob', 'obb', 'aapp', 'ap', 'aap', 'app',\n",
    "             'aaa', 'ooo', 'bbb', 'ppp', 'a', 'o', 'b', 'p', '']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tone plateauing\n",
    "\n",
    "In some of the Bantu languages, the prosodic domain cannot have more than one stretch of H tones. For example, in Luganda (Bantu) a low tone (L) cannot intervene in-between two high tones (H): L is changed to H in such configurations. This pattern is called _tone plateauing_, and its computational properties are discussed in [Jardine (2015,](https://adamjardine.net/files/jardinecomptone-short.pdf) [2016)](https://adamjardine.net/files/jardine2016dissertation.pdf). Consider the following Luganda data from [Hyman and Katamba (2010)](http://linguistics.berkeley.edu/phonlab/documents/2010/Hyman_Katamba_Paris_PLAR.pdf), cited by [Jardine (2016)](https://www.cambridge.org/core/services/aop-cambridge-core/content/view/B01C656A2B96316F3ADCC836BD2A6244/S0952675716000129a.pdf/computationally_tone_is_different.pdf).\n",
    "\n",
    "  * /tw-áa-mú-láb-a, walúsimbi/ $\\Rightarrow$ tw-áá-mu-lab-a, walúsimbi <br>\n",
    "    ‘we saw him, Walusimbi’ <br>\n",
    "    **HHLLL, LHLL**\n",
    "    \n",
    "  * /tw-áa-láb-w-a walúsimbi/ $\\Rightarrow$ tw-áá-láb-wá wálúsimbi <br>\n",
    "    ‘we were seen by Walusimbi’ <br>\n",
    "    **HHHHHHLL**\n",
    "    \n",
    "  * /tw-áa-láb-a byaa=walúsimbi/ $\\Rightarrow$ tw-áá-láb-á byáá-wálúsimbi <br>\n",
    "    ‘we saw those of Walusimbi’ <br>\n",
    "    **HHHHHHHHLL**\n",
    "    \n",
    "Intuitively, this pattern can be generalized as \"make sure that there is no L tone in-between two H tones\".\n",
    "\n",
    "    Good strings: HHLLL, LHHHLL, LLLLHHHH, ...\n",
    "    Bad strings:  <*>LLHLHLLL, <*>HLHLLL, <*>HLLLLLHL, ...\n",
    "    Generalization: no H tone should intervene in-between two L tones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tone_data = [\"LLLL\", \"HHLLL\", \"LHHHLL\", \"LLLLHHHH\", \"HHH\", \"HHHHHLLL\", \"LLLLHH\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Organization of SigmaPie\n",
    "### The functionality of the toolkit\n",
    "\n",
    "The functionality implemened in SigmaPie includes, but is not limited to...\n",
    "\n",
    "  * **learners:** extract grammars from string sets;\n",
    "  * **scanners:** evaluate strings with respect to a given grammar;\n",
    "  * **sample generators:** generate stringsets for a given grammar;\n",
    "  * **FSM constructors:** translate subregular grammars to finite state machines;\n",
    "  * **polarity converters** switch negative grammars to positive, and vice versa."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to run the code\n",
    "\n",
    "#### Way 1: running from the terminal\n",
    "  1. Download the code from the [SigmaPie GitHub folder](https://github.com/alenaks/SigmaPie);\n",
    "  2. Open the terminal and use `cd` to move to the `SigmaPie/code/` repository.\n",
    "  3. Run Python3 compiler by typing `python3`.\n",
    "  4. `from main import *` will load all the modules of the package.\n",
    " \n",
    "  <img src=\"images/terminal.png\" width=\"650\">\n",
    "  \n",
    "#### Way 2: running from the Jupyter notebooks\n",
    "  1. Download the code from the [SigmaPie GitHub folder](https://github.com/alenaks/SigmaPie).\n",
    "  2. Modify the second line in the cell below so that it contains the correct path to `SigmaPie/code/`.\n",
    "  3. Run that cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd\n",
    "%cd SigmaPie/code/\n",
    "\n",
    "from main import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Intuitions behind the implemented subregular classes\n",
    "\n",
    "Grammars can be positive or negative. **Positive grammars** list all allowed substructures of its language, whereas **negative grammars** list the substructures that must not be encountered in well-formed strings of its language. Moreover, these grammars are equivalent, i.e. for every negative grammar, it is possible to construct a positive grammar that generates the same language, and vice versa."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Negative strictly piecewise (`SP`)** grammars prohibit the occurrence of sequences of symbols at an arbitrary distance from each other. Every SP grammar is associated with the value of $k$ that defines the size of the longest sequence that this grammar can prohibit. Alternatively, if the grammar is positive, it lists all subsequences that are allowed in well-formed words of the language. **SP grammars capture only long-distance dependencies that do not include blockers.**\n",
    "\n",
    "    k = 2\n",
    "    POLARITY: negative\n",
    "    GRAMMAR:  ab, ba\n",
    "    LANGUAGE: accaacc, cbccc, cccacaaaa, ...\n",
    "              <*>accacba, <*>bcccacbb, <*>bccccccca, ...\n",
    "\n",
    "**Negative strictly $k$-local (`SL`)** grammars prohibit the occurrence of consecutive substrings consisting of up to $k$ symbols. The value of $k$ defines the longest substring that cannot be present in a well-formed string of a language. Positive SL grammars define substrings that can be present in the language.\n",
    "To define _first_ and _last_ elements, SL languages use delimiters (\">\" and \"<\") that indicate the beginning and the end of the string. In phonology, changes involve adjacent segments very frequently, and the notion of locality is therefore extremely important. A discussion of local processes in phonology can be found in [(Chandlee 2014)](http://dspace.udel.edu/bitstream/handle/19716/13374/2014_Chandlee_Jane_PhD.pdf). **SL grammars only capture local dependencies.**\n",
    "\n",
    "    k = 2\n",
    "    POLARITY: positive\n",
    "    GRAMMAR:  >a, ab, ba, b<\n",
    "    LANGUAGE: ab, abab, abababab, ...\n",
    "              <*>babab, <*>abaab, <*>bababba, ...\n",
    "              \n",
    "**Tier-based strictly local (`TSL`)** grammars operate just like strictly local ones, but they have the power to _ignore_ a certain set of symbols completely. The set of symbols that are not ignored are called **tier** symbols, and the ones that do not matter for the well-formedness of strings are the **non-tier** ones [(Heinz et al. 2011)](https://pdfs.semanticscholar.org/b934/bfcc962f65e19ae139426668e8f8054e5616.pdf). The representation of a string with all non-tier symbols ignored is a _tier image_ of that string, and then the TSL grammars can be defined as _SL grammars that operate over a tier._ **TSL grammars capture a single long-distance dependency that can possibly include blockers.**\n",
    "\n",
    "    k = 2\n",
    "    POLARITY: negative\n",
    "    TIER:     b\n",
    "    GRAMMAR:  ><, bb\n",
    "    LANGUAGE: aaaabaaaa, b, aaaab, baaaa, aaabaaaa, ...\n",
    "              <*>aaaaa, <*>aaaabaabaa, <*>baaabaaa, ...\n",
    "              \n",
    "**Multiple tier-based strictly local (`MTSL`)** grammars are a conjunction of multiple TSL grammars: they consist of several tiers, and a set of restrictions is defined for every one of those tiers. In fact, there are numerous examples from the typological literature showing that there are phonological patterns of complexity which are beyond the power of TSL languages. One example could be any pattern where several long-distance dependencies affect different sets of elements, see [McMullin (2016)](https://www.dropbox.com/s/txmk4efif9f5bvb/McMullin_Dissertation_UBC.pdf?dl=0) and [Aksënova and Deshmukh (2018)](https://www.aclweb.org/anthology/W18-0307.pdf) for examples and discussions of those patterns. **MTSL grammars capture multiple long-distance dependencies that can possibly include blockers.**\n",
    "\n",
    "    k = 2\n",
    "    POLARITY:   negative\n",
    "    TIER_1:     o, ö, u\n",
    "    GRAMMAR_1:  oö, öo, uö\n",
    "    TIER_2:     p, b\n",
    "    GRAMMAR_2:  bp\n",
    "    LANGUAGE:   obobo, öpöpbbu, öpuupobbo, opuopo, ...\n",
    "                <*>öbuuupoo, <*>oobbböb, <*>poobböp, ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attributes implemented for grammars\n",
    "\n",
    "Languages ($L$) in the _SigmaPie_ toolkit are defined as objects initialized with the following attributes:\n",
    "\n",
    "  * `L.polar` is the polarity of the grammar, and this attribute is only available upon initialization: `L.switch_polarity(new_value)` needs to be used to change it later, because it makes sure that the grammar is converted to a new polarity as well (default: positive);\n",
    "  * `L.alphabet` is a set of symbols used in the language of the grammar;\n",
    "  * `L.grammar` lists allowed or banned sequences;\n",
    "  * `L.k` defines the locality window of the grammar;\n",
    "  * `L.data` is a training sample;\n",
    "  * `L.fsm` corresponds to a finite state device or devices that correspond to the grammar;\n",
    "  * `L.edges` lists the delimiters implemented in the grammar (not relevant for SP);\n",
    "  * `L.tier` is a list or lists of tier symbols (not relevant for SP and SP).\n",
    "\n",
    "\n",
    "| Attributes         | SP | SL | TSL | MTSL |\n",
    "|----------|----|----|-----|------|\n",
    "| `L.polar`    | $\\surd$  | $\\surd$  | $\\surd$   | $\\surd$    |\n",
    "| `L.alphabet` | $\\surd$  | $\\surd$  | $\\surd$   | $\\surd$    |\n",
    "| `L.grammar`  | $\\surd$  | $\\surd$  | $\\surd$   | $\\surd$    |\n",
    "| `L.k`        | $\\surd$  | $\\surd$  | $\\surd$   | $\\surd$    |\n",
    "| `L.data`     | $\\surd$  | $\\surd$  | $\\surd$   | $\\surd$    |\n",
    "| `L.fsm`      | $\\surd$  | $\\surd$  | $\\surd$   | $\\surd$    |\n",
    "| `L.edges`    | $\\neg\\exists$  | $\\surd$  | $\\surd$   | $\\surd$    |\n",
    "| `L.tier`     | $\\neg\\exists$  | $\\neg\\exists$  | $\\surd$   | $\\surd$    |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Methods implemented for grammars\n",
    "\n",
    "The following methods are available in the toolkit for every language class $L$:\n",
    "\n",
    "  * `L.learn()` extracts the grammar from the provided training sample in `L.data`;\n",
    "  * `L.scan(string)` verifies if the `string` is well-formed with respect to `L.grammar` or not;\n",
    "  * `L.check_polarity()` returns the `L.polar` of the grammar;\n",
    "  * `L.switch_polarity(new_value)` changes the polarity of the grammar to `new_value`, if no arguments are provided, it changes to the opposite polarity;\n",
    "  * `L.extract_alphabet()` fills the `L.alphabet` attribute by determining it based on `L.data` or `L.grammar`;\n",
    "  * `L.generate_all_ngrams(alphabet, n)` generates a list of all possible n-grams (length `n`) for the list of symbols in `alphabet`;\n",
    "  * `L.generate_sample(n, repeat, safe)` randomly generates `n` strings that are well-formed with respect to `L.grammar`, `repeat` allows or prohibits repeating the same generated strings, and `safe` detects cases when $n$ strings cannot be generated, i.e. the grammar generates a finite language with a size less than $n$ (safe mode is on by-default);\n",
    "  * `L.fsmize()` creates a FSM or a collection of FSMs that correspond to `L.grammar`;\n",
    "  * `L.clean_grammar()` detects uninformative elements of `L.grammar` and removes them;\n",
    "  * `L.tier_image(string)` returns a tier image or a list of tier images of the given `string` (relevant for TSL and MTSL);\n",
    "  * `L.subsequences(string)` returns all possible `L.k`-long subsequences of `string`.\n",
    "  \n",
    "| Methods             | SP | SL | TSL | MTSL |\n",
    "|---------------------|----|----|-----|------|\n",
    "| `learn`               | $\\surd$  | $\\surd$  | $\\surd$   | $\\surd$    |\n",
    "| `scan`                | $\\surd$  | $\\surd$  | $\\surd$   | $\\surd$    |\n",
    "| `check_polarity`      | $\\surd$  | $\\surd$  | $\\surd$   | $\\surd$    |\n",
    "| `switch_polarity`     | $\\surd$  | $\\surd$  | $\\surd$   | $\\surd$    |\n",
    "| `extract_alphabet`    | $\\surd$  | $\\surd$  | $\\surd$   | $\\surd$    |\n",
    "| `generate_all_ngrams` | $\\surd$  | $\\surd$  | $\\surd$   | $\\surd$    |\n",
    "| `generate_sample`     | $\\surd$  | $\\surd$  | $\\surd$   | $\\surd$    |\n",
    "| `fsmize`              | $\\surd$  | $\\surd$  | $\\surd$   | $\\surd$    |\n",
    "| `clean_grammar`       | $\\surd$  | $\\surd$  | $\\surd$   | $\\surd$    |\n",
    "| `tier_image`          | $\\neg\\exists$  | $\\neg\\exists$  | $\\surd$   | $\\surd$    |\n",
    "| `subsequences`        | $\\surd$  | $\\neg\\exists$  | $\\neg\\exists$   | $\\neg\\exists$    |\n",
    "\n",
    "\n",
    "**Learning algorithms:**\n",
    "  * **_k_-SL** and **_k_-SP** learning strategies are explained by [Heinz (2010)](http://jeffreyheinz.net/papers/Heinz-2010-SEL.pdf);\n",
    "  * **_k_TSLIA** is a learning algorithm for $k$-TSL languages, designed by [McMullin and Jardine (2017)](https://adamjardine.net/files/jardinemcmullin2016tslk.pdf), which in turn is based on [Jardine and Heinz (2016)](http://jeffreyheinz.net/papers/Jardine-Heinz-2016-LTSLL.pdf).\n",
    "  * **MTSL2IA** is a learning algorithm for $2$-MTSL languages, developed by [McMullin et al. (2019)](https://scholarworks.umass.edu/cgi/viewcontent.cgi?article=1079&context=scil). Currently we are working on extending this algorithm to an arbitrary window size.\n",
    "\n",
    "\n",
    "These methods enable a wide variety of ways to use the _SigmaPie_ toolkit.\n",
    "For example, `L.scan(string)` checks if the current grammar still works with an updated or held-out dataset.\n",
    "`L.generate_sample(n, repeat, safe)` can be used to generate an arbitrary-sized dataset for artificial grammar learning experiments, or neural models. Finally, if the grammar is hand-written, `L.clean_grammar()` will detect and remove its purposeless elements."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generalizing the harmonic pattern\n",
    "\n",
    "The harmonic pattern involves two independent long-distance assimilations.\n",
    "\n",
    "    Good strings: aaabbabba, oppopooo, aapapapp, obooboboboobbb, ...\n",
    "    Bad strings:  <*>aabaoob, <*>paabab, <*>obabooo, ...\n",
    "    Generalization: if a string contains \"a\", it cannot contain \"o\", and vice versa;\n",
    "                    if a string contains \"p\", it cannot contain \"b\", and vice versa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "harm_data = ['aabbaabb', 'abab', 'aabbab', 'abaabb', 'aabaab', 'abbabb', 'ooppoopp',\n",
    "             'opop', 'ooppop', 'opoopp', 'oopoop', 'oppopp', 'aappaapp', 'apap',\n",
    "             'aappap', 'apaapp', 'aapaap', 'appapp', 'oobboobb', 'obob', 'oobbob',\n",
    "             'oboobb', 'ooboob', 'obbobb', 'aabb', 'ab', 'aab', 'abb', 'oopp', 'op',\n",
    "             'oop', 'opp', 'oobb', 'ob', 'oob', 'obb', 'aapp', 'ap', 'aap', 'app',\n",
    "             'aaa', 'ooo', 'bbb', 'ppp', 'a', 'o', 'b', 'p', '']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step is to initialize $4$ subregular grammars for every one of the available classes and provide the training sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sp_h = SP()\n",
    "sl_h = SL()\n",
    "tsl_h = TSL()\n",
    "mtsl_h = MTSL()\n",
    "\n",
    "sp_h.data = harm_data\n",
    "sl_h.data = harm_data\n",
    "tsl_h.data = harm_data\n",
    "mtsl_h.data = harm_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then simply fill the `L.alphabet` attributes by applying the method `L.extract_alphabet()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sp_h.extract_alphabet()\n",
    "sl_h.extract_alphabet()\n",
    "tsl_h.extract_alphabet()\n",
    "mtsl_h.extract_alphabet()\n",
    "\n",
    "print(\"SP alphabet:\", sp_h.alphabet)\n",
    "print(\"SL alphabet:\", sl_h.alphabet)\n",
    "print(\"TSL alphabet:\", tsl_h.alphabet)\n",
    "print(\"MTSL alphabet:\", mtsl_h.alphabet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning\n",
    "\n",
    "After the data and alphabet are established, we can extract the grammar with its corresponding complexity from every one of those classes using the method `L.learn()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sp_h.learn()\n",
    "sl_h.learn()\n",
    "tsl_h.learn()\n",
    "mtsl_h.learn()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The learned **SP grammar** lists all possible $2$-long subsequences observed in the training sample. The sequences are represented as tuples instead of strings in order to avoid restricting the basic alphabet units to a single symbol."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Positive SP grammar:\", sp_h.grammar)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to express the same generalization using a negative grammar, we can `L.switch_polarity()` of the grammar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sp_h.switch_polarity()\n",
    "print(\"Polarity of the SP grammar:\", sp_h.check_polarity())\n",
    "print(\"SP grammar:\", sp_h.grammar)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The negative **SL grammar** contains the same set of restrictions as its SP counterpart. So, the same set of restrictions is detected, even though SP grammars express long-distance restrictions, whereas SL grammars only limit local relations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sl_h.switch_polarity()\n",
    "print(\"Polarity of the SL grammar:\", sl_h.check_polarity())\n",
    "print(\"SL grammar:\", sl_h.grammar)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TSL grammars** also express the same limitations over a tier as the previous two, and the tier includes all elements that participate in some sort of long-distance agreement. In this case, the tier includes all elements of the alphabet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"TSL tier:\", tsl_h.tier)\n",
    "tsl_h.switch_polarity()\n",
    "print(\"Polarity of the TSL grammar:\", tsl_h.check_polarity())\n",
    "print(\"TSL grammar:\", tsl_h.grammar)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In **MTSL grammars**, the value of the attribute `L.grammar` is represented in the following way:\n",
    "\n",
    "    G = {\n",
    "            tier_1 (tuple): tier_1_restrictions (list),\n",
    "            tier_2 (tuple): tier_2_restrictions (list),\n",
    "                ...\n",
    "            tier_n (tuple): tier_n_restrictions (list)\n",
    "        }\n",
    "        \n",
    "The learned grammar detected two tiers: a tier of vowels `(\"a\", \"o\")` and a tier of consonants `(\"p\", \"b\")`. For every one of these tiers, it learned the corresponding set of restrictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"MTSL tiers:\", mtsl_h.tier)\n",
    "mtsl_h.switch_polarity()\n",
    "print(\"Polarity of the MTSL grammar:\", mtsl_h.check_polarity())\n",
    "print(\"MTSL grammar:\", mtsl_h.grammar)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating new strings\n",
    "\n",
    "Now, when all the grammars are learned, we can generate new data using the `L.generate_sample(n, repeat, safe)` method.\n",
    "\n",
    "The **SP-generated** data is consistent with the desired pattern: no \"a\" is followed by \"o\" in the generated strings of the language, and the consonantal agreement is preserved as well. SP grammar succeeded in capturing the pattern!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sp_h.generate_sample(25, repeat=False, safe=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see below, **SL grammar** captured the local effect of the learned pattern (\"p\" is never adjacent to \"b\", \"o\" is never adjacent to \"a\", etc.), but it failed to generalize it to long-distance relations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sl_h.generate_sample(25, repeat=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly to the errors made by the SL grammar, **TSL grammar** only captured a local dependency, and failed to generalize the pattern."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tsl_h.generate_sample(25, repeat=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Increasing the locality window `L.k` did not help solve the issue: violations still occur, the only difference is that now they occur at a further distance away from each other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsl_h.k = 3\n",
    "tsl_h.learn()\n",
    "print(tsl_h.generate_sample(25, repeat=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And, as the next cell exemplifies, the **MTSL grammar** also successfully captured the double harmony pattern. Intuitively, MTSL grammar learned that on the tier of vowels, \"o\" and \"a\" should never be adjacent, and on the tier of consonants, restrictions \"pb\" and \"bp\" need to be imposed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(mtsl_h.generate_sample(25, repeat=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overall, we can see that two grammars (SP and MTSL) successfully handled the double harmonic pattern.\n",
    "\n",
    "  * **SP solution:** within the same word, there cannot be two different vowels and two different consonants;\n",
    "  * **MTSL solution:** if we only look at vowels, all vowels that are adjacent to each other need to be the same, and similarly for consonants."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generalizing the tone plateauing pattern\n",
    "\n",
    "The tone plateauing process ensures that there is no L tone between two H tones.\n",
    "\n",
    "    Good strings: HHLLL, LHHHLL, LLLLHHHH, ...\n",
    "    Bad strings:  <*>LLHLHLLL, <*>HLHLLL, <*>HLLLLLHL, ...\n",
    "    Generalization: no H tone should intervene in-between two L tones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tone_data = ['', 'LLLHL', 'HHLLLL', 'LLHHLL', 'HLL', 'HHHHHHHL', 'LHHHH', 'LH', 'HHL', 'HH',\n",
    "             'HL', 'LLLHLLLLL', 'LHL', 'LLLLHLLLL', 'HHHHHH', 'HHHHH', 'LLLHLLLL', 'HHHHL',\n",
    "             'HLLLLL', 'LLL', 'LHLLLL', 'L', 'LLLLL', 'HHH', 'HLLLL', 'HHHH', 'HHLLL',\n",
    "             'LLLLLLLLH', 'HHHLL', 'LLHHHHH', 'LLLHHHHL', 'LLHL', 'LHHHL', 'LLLLH', 'LL',\n",
    "             'HHLL', 'HHHLLLL', 'LHH', 'LHHLLL', 'HLLL', 'LHHH', 'LHLL', 'H', 'LLLHHHL', 'HHHL',\n",
    "             'LLHLL', 'HHHHLL', 'LLH', 'HLLLLLLL', 'LHHLLLLL']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the first step, let us directly initialize negative SP, SL, TSL and MTSL grammars."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sp_tp = SP(polar='n')\n",
    "sl_tp = SL(polar='n')\n",
    "tsl_tp = TSL(polar='n')\n",
    "mtsl_tp = MTSL(polar='n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is, as before, to provide the input data to language objects, and to extract the alphabets automatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sp_tp.data = tone_data\n",
    "sp_tp.extract_alphabet()\n",
    "\n",
    "sl_tp.data = tone_data\n",
    "sl_tp.extract_alphabet()\n",
    "\n",
    "tsl_tp.data = tone_data\n",
    "tsl_tp.extract_alphabet()\n",
    "\n",
    "mtsl_tp.data = tone_data\n",
    "mtsl_tp.extract_alphabet()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this pattern, we will need to increase the locality window of the grammar to $3$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sp_tp.k = 3\n",
    "sl_tp.k = 3\n",
    "tsl_tp.k = 3\n",
    "mtsl_tp.k = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning\n",
    "\n",
    "Similarly, we can now generalize the tone plateauing pattern with respect to those grammars."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sp_tp.learn()\n",
    "sl_tp.learn()\n",
    "tsl_tp.learn()\n",
    "mtsl_tp.learn()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **SP grammar** learned the desired pattern, i.e. no \"HLH\" subsequence must be found across the input string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Negative SP grammar:\", sp_tp.grammar)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**SL, TSL and MTSL** seem to detect the same restriction as well: \"HLH\" is prohibited by every one of those grammars."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Negative SL grammar:\", sl_tp.grammar)\n",
    "print(\"Negative TSL grammar:\", tsl_tp.grammar)\n",
    "print(\"Negative MTSL grammar:\", mtsl_tp.grammar)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating new strings\n",
    "\n",
    "Now, when all initialized languages have their grammar learned, we can try to generate new data to see if it will still be consistent with the rule of tone plateauing, i.e. \"don't have L tones in-between H tones\".\n",
    "\n",
    "The **SP grammar** consistently predicts correct forms: it allows the occurrence of H tones between L tones, but not the other way around."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"SP sample:\", sp_tp.generate_sample(20, repeat=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, none of the **local grammars** generalized the pattern correctly. Even though we do not see the \"HLH\" substring locally in any of the strings generated by those grammars, we observe the illicit substructure spread across some of those strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"SL sample:\", sl_tp.generate_sample(20, repeat=False), \"\\n\")\n",
    "print(\"TSL tier:\", tsl_tp.tier)\n",
    "print(\"TSL sample:\", tsl_tp.generate_sample(20, repeat=False), \"\\n\")\n",
    "print(\"MTSL tiers:\", mtsl_tp.tier)\n",
    "print(\"MTSL sample:\", mtsl_tp.generate_sample(20, repeat=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Only the SP grammar was able to correctly generalize the tone plateauing pattern. The logic of that grammar is \"nowhere in the string can a low tone be simultaneously followed and preceded by a high tone\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grammar learning exercise: liquid dissimilation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In several languages (Latin, Georgian, a.o.), liquids tend to alternate. \n",
    "Consider the following Latin data: if the final liquid of the stem is \"l\", the adjectival affix is realized as \"aris\". And vice versa, if the final liquid is \"r\", the choice of the affix is \"alis\".\n",
    "\n",
    "  * mi<b>l</b>ita<b>r</b>is \\~ <*>mi<b>l</b>ita<b>l</b>is _\"military\"_\n",
    "  * f<b>l</b>o<b>r</b>a<b>l</b>is \\~ <*>f<b>l</b>o<b>r</b>a<b>r</b>is _\"floral\"_\n",
    "  * p<b>l</b>u<b>r</b>a<b>l</b>is \\~ <*>p<b>l</b>u<b>r</b>a<b>r</b>is _\"plural\"_\n",
    "  \n",
    "We can simplify this pattern by mapping the liquids to themselves, and any intervening vowel or consonant to `c`:\n",
    "\n",
    "    Good strings: clcccrccl, ccrccclcccrclc, lccccrlcccrcclccc, ...\n",
    "    Bad strings:  <*>ccrcccrccl, <*>clccrcr, <*>lccrccrc, ...\n",
    "    Generalization: the liquid following \"r\" needs to be \"l\", and vice versa;\n",
    "                    \"c\" is irrelevant for the ordering of the liquids. \n",
    "                    \n",
    "                    \n",
    "The following dataset exemplifies the pattern of liquid dissimilation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "liquid_data = [\"\", \"ccc\", \"lccrcccclcr\", \"lrl\", \"rcclc\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the cell below to explore which language type generalizes the liquid dissimilation pattern the best.\n",
    "\n",
    "**NB:** an improved version of the MTSL learning algorithm is coming soon!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Future work\n",
    "\n",
    "Formal languages and their corresponding finite-state acceptors map strings to truth values. They answer the question **\"Is this string well-formed according to the given grammar?\"** This question helps to define the well-formedness conditions for _phonotactics_.\n",
    "\n",
    "However, to capture _phonological processes_, we need to also ask\n",
    "**\"What string will be the output be if we process the input string according to the given mapping?\"** Subregular mappings and finite-state transductions map strings to strings, so they can help us with finding the answer to this question.\n",
    "\n",
    "<img src=\"images/scheme.png\" width=\"400\">\n",
    "\n",
    "Thus, the next steps of the development of _SigmaPie_ include the implementation of transducers and different transduction learning algorithms, such as:\n",
    "  * Onward Subsequential Transducer Inference Algorithm (_OSTIA_) by [Oncina, Garcia and Vidal (1993)](https://pdfs.semanticscholar.org/9058/01c8e75daacb27d70ccc3c0b587411b6d213.pdf) and [de la Higuera (2014)](https://www.cambridge.org/core/books/grammatical-inference/CEEB229AC5A80DFC6436D860AC79434F);\n",
    "  * Input Strictly Local Function Learning Algorithm (_ISLFLA_) by [Chandlee, Eyraud and Heinz (2014)](https://hal.archives-ouvertes.fr/hal-01193047/document);\n",
    "  * Output Strictly Local Function Inference Algorithm (_OSLFIA_) by [Chandlee, Eyraud and Heinz (2015)](https://www.aclweb.org/anthology/W15-2310.pdf)\n",
    "  \n",
    "... and others."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Suggestions** \n",
    "\n",
    "If your research can benefit in any way from the extension of _SigmaPie_, please let me know by shooting me an email at <alena.aksenova@stonybrook.edu>!\n",
    "\n",
    "\n",
    "**Acknowledgments** \n",
    "\n",
    "I am very grateful to [_Thomas Graf_](https://thomasgraf.net/), [_Jeffrey Heinz_](http://jeffreyheinz.net/), [_Aniello De Santo_](https://aniellodesanto.github.io/about/) and _Ayla Karakaş_ whose input on different parts of this project was extremely helpful."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
